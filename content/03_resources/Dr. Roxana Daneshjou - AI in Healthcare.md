---
title: "Video Conferencing, Web Conferencing, Webinars, Screen Sharing"
source: "https://stanford.zoom.us/rec/play/nf0Td4gEDASqKVHfaOTIuWJcD8f1Dl9MtsTQ0jfkIpw3crsKOuwaKtPs2Xol0_babkjudUvXwc2Hb44.l3soSr0wSCgnYe4p?eagerLoadZvaPages=&accessLevel=meeting&canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fstanford.zoom.us%2Frec%2Fshare%2F5iW7sx-AIKbhr5HilwpIbNqLY6fYEExV01AoeizOzIikC-GwMCXyZR3zlrUx5L99.QJJWsI5zI1OZZPEo"
author:
  - "[[Zoom]]"
published:
created: 2025-07-04
description: "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference room solution used around the world in board, conference, huddle, and training rooms, as well as executive offices and classrooms. Founded in 2011, Zoom helps businesses and organizations bring their teams together in a frictionless environment to get more done. Zoom is a publicly traded company headquartered in San Jose, CA."
tags:
  - "clippings"
---
## Summary
Dr. Roxana Daneshjou, a dermatologist at Stanford, discusses the application of AI in healthcare, highlighting both its potential and significant challenges. She shares her career path from bioengineering to an MD-PhD and post-doctoral work in AI. Dr. Daneshjou explains the fundamental issues in the US healthcare system, such as access and inefficiency, which AI aims to address. She then provides a high-level overview of how large language models (LLMs) and vision language models (VLMs) are designed, emphasizing concepts like transformers, self-supervised learning, and reinforcement learning with human feedback. A significant portion of her talk focuses on the ethical pitfalls of AI in medicine, including biases (e.g., perpetuating racist medical ideas, inaccurate diagnoses across skin tones) and the risk of automation bias, where users over-rely on AI despite potential errors. She advises caution when using AI for new information and stresses the importance of diverse and representative training data. Dr. Daneshjou also touches on the regulation of medical AI by the FDA and the evolving job market in medicine, suggesting shifts in roles rather than job losses.

## Key Points
- **Speaker Background:** Dr. Roxana Daneshjou, a dermatologist at Stanford, with an academic background in bioengineering, MD-PhD, and AI postdoc.
- **Motivation for AI in Healthcare:** Addresses fundamental issues in healthcare, such as access difficulties, physician shortages, and system inefficiency.
- **Large Language Models (LLMs):**
    - **Design:** Based on transformer architecture (e.g., 'Attention is all you need' paper).
    - **Training Regimen:** Self-supervised learning (predicting masked words), reinforcement learning with human feedback (human preference training via thumbs up/down), and prompting (e.g., chain of thought, role-playing).
    - **Usage:** Widely used by physicians (65% in a 2024 study, daily/weekly for patient education, administrative tasks) and patients (90% for personal health questions).
- **Vision Language Models (VLMs):**
    - **Training:** Uses contrastive learning to map images and text into a shared mathematical representation.
    - **Application:** Generating captions, image generation, and diagnostic assistance (e.g., dermatology).
- **Challenges and Drawbacks of AI in Healthcare:**
    - **Bias:** Models can perpetuate false, racist ideas in medicine (e.g., kidney function, pain tolerance) and show diagnostic bias across different skin tones.
    - **Inaccuracy/Hallucinations:** AI models can provide incorrect information, especially for complex tasks like medication dosages or specific medical calculations, and these errors can be hard to detect.
    - **Automation Bias:** Over-reliance on automated aids, leading users to override their own judgment (e.g., GPS leading drivers into water).
    - **Data Gaps:** Lack of sufficient and representative data from under-resourced regions exacerbates biases.
- **Regulation and Trust:**
    - Medical AI tools are regulated by the FDA, though the field's rapid pace makes regulation challenging.
    - Clinician trust in predictive AI models is still developing; AI is currently more useful as a supplementary tool (e.g., assisting in colonoscopies) rather than a sole decision-maker.
- **Job Market Impact:** AI will likely cause shifts in how medical professionals work, rather than leading to widespread job loss, by automating certain workflows and creating new specialties.
- **Addressing Bias:** Requires appropriate data collection to ensure representation, and techniques like reinforcement learning and explainable AI to understand and mitigate biased behaviors. Local data is crucial for model accuracy in specific regions.

---

https://stanford.zoom.us/rec/play/nf0Td4gEDASqKVHfaOTIuWJcD8f1Dl9MtsTQ0jfkIpw3crsKOuwaKtPs2Xol0_babkjudUvXwc2Hb44.l3soSr0wSCgnYe4p?eagerLoadZvaPages=&accessLevel=meeting&canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fstanford.zoom.us%2Frec%2Fshare%2F5iW7sx-AIKbhr5HilwpIbNqLY6fYEExV01AoeizOzIikC-GwMCXyZR3zlrUx5L99.QJJWsI5zI1OZZPEo
# Original Content

[Accessibility Overview](https://stanford.zoom.us/en/accessibility)

<video src="https://ssrweb.zoom.us/replay02/2025/06/20/C252707A-9A70-43E5-A50E-CA8D322AA84E/GMT20250620-194825_Recording_1920x1080.mp4?response-content-type=video%2Fmp4&amp;response-cache-control=max-age%3D0%2Cs-maxage%3D86400&amp;data=ba89b7a4df6f0b14f6c49cce8fafe06ecf7a69d8d0c89f91def92d21f26953e0&amp;s001=yes&amp;cid=aw1&amp;fid=LJMGHSrTUW8pFWCznbgRUdDjXx6ijp5vRDT5PMGb-h_Wlh_ycKx18D26NGQHhqt6XkxEgQPMShL4vMo.SKoh69NaRCCda1Ar&amp;s002=g4gVpEi2MzW7v7ePdhuxmXhpSnMPFa_Ld8rtz8l9BHrQ2vQXALOKmhHY2ZZ5xYUfFunH_uFNhoyDO_FQfleXXKAEouXh.8Gg22xvyVqQqlYBd&amp;tid=v=2.0;clid=aw1;rid=WEB_f2b5f76f82d07100df13c958e856d852&amp;Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6Imh0dHBzOi8vc3Nyd2ViLnpvb20udXMvcmVwbGF5MDIvMjAyNS8wNi8yMC9DMjUyNzA3QS05QTcwLTQzRTUtQTUwRS1DQThEMzIyQUE4NEUvR01UMjAyNTA2MjAtMTk0ODI1X1JlY29yZGluZ18xOTIweDEwODAubXA0P3Jlc3BvbnNlLWNvbnRlbnQtdHlwZT12aWRlbyUyRm1wNCZyZXNwb25zZS1jYWNoZS1jb250cm9sPW1heC1hZ2UlM0QwJTJDcy1tYXhhZ2UlM0Q4NjQwMCZkYXRhPWJhODliN2E0ZGY2ZjBiMTRmNmM0OWNjZThmYWZlMDZlY2Y3YTY5ZDhkMGM4OWY5MWRlZjkyZDIxZjI2OTUzZTAmczAwMT15ZXMmY2lkPWF3MSZmaWQ9TEpNR0hTclRVVzhwRldDem5iZ1JVZERqWHg2aWpwNXZSRFQ1UE1HYi1oX1dsaF95Y0t4MThEMjZOR1FIaHF0NlhreEVnUVBNU2hMNHZNby5TS29oNjlOYVJDQ2RhMUFyJnMwMDI9ZzRnVnBFaTJNelc3djdlUGRodXhtWGhwU25NUEZhX0xkOHJ0ejhsOUJIclEydlFYQUxPS21oSFkyWlo1eFlVZkZ1bkhfdUZOaG95RE9fRlFmbGVYWEtBRW91WGguOEdnMjJ4dnlWcVFxbFlCZCZ0aWQ9dj0yLjA7Y2xpZD1hdzE7cmlkPVdFQl9mMmI1Zjc2ZjgyZDA3MTAwZGYxM2M5NThlODU2ZDg1MiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTY0Mjk2NX19fV19&amp;Signature=ZZQKltlz7-jyuk3DOqV8QBEWvHTT-tzRCxuFXe7fJYXYxmvrOag9Vanb1SqWHEbDgLgv0Pzexv4PMiOs986bRhnRfHvIHDsEyYLtyT4F-uR~Bv-YY9~bzBaBQ0kFcNv4sWJ7wauDxFHfcvH66kbizA99owltpp8SbAgKHOo~MMD-7iF0oGL-60BeTpe-T9jt3LJ8JTsZhXvdbBPoIAfOlTyBaqNGzmr0pzRzW31ijRzJ~PmjSTSGVV1ZxYZJgmNzl5gjCgKgR9k4TFFTSLg7H5KmZ5T7uhmWGeRwkT4EEODWIM33JkZuDJsy8~su4cTlsH2lPP5wfYG726JlXCyFUQ__&amp;Key-Pair-Id=APKAJFHNSLHYCGFYQGIA"></video>

In dermatology at Stanford. Um, it's actually, like, half a day a week, and it'sâ€¦ And it's 33 clinics a year.

00:00:00 / 00:54:04

## Audio Transcript

## Chat Messages

- 00:03
	Clinics a year.
- So it's actually a smaller load than weekly clinic.
- And then how did I get here? Because I figured you guys are in high school and might want to know. So, as we've mentioned, I did my undergrad in bioengineering. I did this Md. Phd. Program which a lot of medical schools
- have which is 2 years of Md. 3 years of Phd.
- And then the last 2 years of Md.
- Then I did a medical Residency, where you do more medical training.
- I did one year of internal medicine, 2 years of dermatology, and then one year in the research track. And then I did 2 years after that
- as a postdoc in AI, so you can do the math
- that it was a very long time. My undergrad was a bit shorter. I did finish undergrad in 3 years. So
- that was like maybe the only part where I got to like shave a little bit of time off.
- So why do people say that we should be applying AI to healthcare? The problem is is that healthcare is actually fundamentally broken? Especially in the United States. I know people come from all over, but like in the Us like.
- I always like to say, like, let's say you're at the beach enjoying your day, and your friend notices this spot on your back and says, What's this?
- The process by which you would get in to see a doctor. It might take weeks to see your primary care, doctor.
- or like your family doctor.
- but those who have understand that it's just
- very difficult to get access to care because their physician shortages. The system is very inefficient.
- And so people have thought, all right. Well, what if we have like AI models that can help like streamline things like help with triaging patients, help with doctors doing their work in a more efficient way.
- And in fact, there's been work to bring, like Gpt for Ehr's electronic health record system. So epic is the largest electronic Health Record System Company
- in the Us. It's how like we're physicians. Document Google has been testing their chat Bot with the Mayo clinic here. So you know, generative AI like has a lot of promise for healthcare.
- So today we'll talk about what at a high level
- how large language models are designed, understand, at a high level how vision language models are designed and then understand, like some of these potential drawbacks.
- that these models have.
- Okay? So 1st off, I'm just curious.
- and you can drop a note in the chat. How many of you use language? Large language models?
- Okay, it's like everyone saying, me,
- what do you use it for?
- Want to see homework, generating some lots of homework, programming, helping with study guys, just asking everything to learn stuff, either. Look at answer questions, organizing research, code.
- fun, school creating practices, cramming.
- Well, this is really fun to see what people are using this for miscellaneous stuff.
- Friendship, random coding, cool
- opportunities. Okay, next question, how many of you know.
- hate zoom for this. How large language models work.
- feel comfortable saying they know how how the model was built.
- Kind of, yeah, I think so a bit a little ish.
- Okay, we'll talk about that a bit sort of sometimes somewhat. Kinda yeah. I think
- I love all the different ways people are saying somewhat, okay. So we're gonna talk about it a bit
- on a high level.
- I always joke to people. I'm like.
- actually, not many people know exactly how any one model was made because the companies don't release that information, and so each of them is like a sausage that you don't know what's exactly went into the sausage, because many of the companies don't even release what training data they used or what the different procedures they'll give it to at a heart high level.
- how many of you guys have heard of this paper called attention is all you need.
- I looked at it a bit before. Okay, so this is about transformers, which is an architecture. That really kind of helped set up
- what we have as the what has the the. This architecture
- helped set up like the modern day, large language model.
- And I'm not gonna go into detail of exactly how it works. But
- like like, I'm not gonna go into a deep dive because there are whole courses taught on this.
- But I'm just gonna like, Do a sentence for you. And and the sentence is this, I want you guys to complete it in the chat? Okay?
- So I I went to on a trip to Paris.
- and I had a fantastic time. I ate a lot of delicious food. I went shopping at all my favorite stores. I got to see all of the beautiful sights.
- and I got to practice speaking blank. What's blank?
- Okay, everyone's saying French, okay, congratulations. You're all really good at being a a language model? Essentially.
- before transformers, you know. Why, what? Like? Why did you guys know that I was speaking French?
- Because I was in Paris? Yeah, exactly. You're like, that's the context.
- So before transformers the architectures that language models were built upon had a really difficult time
- seeing Pat past the context
- You know things.
- But you guys all remembered, even though, that I crammed in a bunch of
- stuff in there, that I was in Paris. And so I must be speaking French.
- and that is the difference that
- you know. The transformer architecture was able to solve.
- It breaks, you know, like anything else. It breaks
- words up into tokens, and so large words like pharmacogenomics.
- get broken up is a study of so like if we I should make a different one of these with my
- my Paris sentence, and it's able to learn sort of which words are important
- for the specific context. Even if that word is far away
- from the word that you care to predict. And so this is like probabilities for a different sentence.
- And this is the temperature that tells you how stochastic you want to be.
- Basically, usually these large language models don't always they do not regularly spit out the most high probability word they pick somewhere amongst the high probability. And so you may have noticed that if you ask the language, model the same question exact same question 2 times, it gives you slightly different answers, because it's not always putting out the same, you know.
- Highest probability word. So I'm going to go through now. The training regimen for the model. The 1st
- step pre-training is something called self supervised learning.
- And so what it's doing is it's like playing a game with itself. It's taking large volumes of text.
- sentences out of it, and blanking out different parts of the sentences, and playing a game where it tries to predict what that word should be. So if I said. The dog became excited, and ran across the grass to chase the blank. What would you fill the blank in with
- ball, squirrel, cat, ball, squirrel, bird
- mouse. So like even you guys will see like the highest probability word, for here is ball. But then Squirrel is pretty aardvark. I like whoever, said aardvark. Thank you.
- my point is is, that's actually exactly how, when these language models play this game, how it works, because they're seeing okay, every time I see about dogs chasing the highest probability. Word is ball right, but also squirrel appears with some probability. Mouse aardvark is probably low probability.
- and is basically able to re like learn relationships amongst words.
- And then, after that, what happens is
- so at this point, no human has really been involved in training the model, saying, Hey, you're learning the wrong association, the Right association.
- What happens next is something called reinforcement learning with human feedback. Where?
- so that you're actually training the model on human preference.
- That's what we call the reinforcement learning with with human feedback.
- And then, finally, there's prompting.
- So some of you probably have experience with some prompting. So like, for example, if you give it a math problem
- and just give it like, you've an example math problem with the answer and then ask it another math problem, it actually can get the wrong answer. If you prompt it
- to give it a math problem.
- then actually tell it how to solve that math problem
- and then ask another math problem.
- It gets the right answer. That's chain of thought, prompting
- other ways of prompting us by saying like things like, you know, pretend you're like a Shakespearean bard writing it, or please write this like in the style of the rapper, some rapper, or whatever
- and that is that by prompting it you are
- actually kind of shaping how you want its output to look
- And actually, I had a friend who did this really interesting study where they had models, pretend to be different kinds of doctors, and the opinions that the model reflected in each case was like
- what a neurologist would think versus an immunologist or psychiatrist or pediatrician. So have you guys played around with prompting the models.
- Tell it to write as professional. Yes.
- I tell it my grade. So it knows what level to speak at. Give them a role.
- Yeah, also giving them examples.
- So something you know, something called in context. You know.
- in context, learning, it's like where you give it examples, try to focus it. Explain this lesson like you were a teenage girl.
- That's that's funny.
- Yep, giving them examples and having it solve some of that's in context. Learning.
- Okay.
- large language models, particularly dermatologists, because we were very interested in this. And what we found
- out of like surveying a hundred ended up getting a hundred, 48
- physicians across all kinds of practice settings
- is, and this studies from 2024, which is.
- you know, I think these numbers are even high, like, like 65% have used large language models in clinic for like clinical use.
- Many of them were using it daily or weekly.
- and they're using it for like patient education, patient care, administrative tasks, medical records, and others, and
- accuracy, like most people, thought that it was somewhat accurate
- and you know, good portion have to do like moderate edits.
- We also looked at 40 individuals
- looking like what a patient's doing. Like 90%. We're using Llms in general, and 90% have used for their personal health.
- you know, and many of them have done it for like 5 or more times.
- you know, just a large number of people are using it for healthcare. And most people are using chat Gpt, but some are using gemini anthropic
- and many said, they'll do it again.
- And patients may not tell their doctors that they're using these models?
- So before I kind of go into the next point, I'm curious.
- How many have have people ever used large language models here to ask health questions.
- Oh, wow, yeah, for school.
- What about? Yeah. Health class? Okay, okay,
- now, we're gonna talk about some problems. So we did this study
- because physicians were using it to see?
- If so, there are these ideas that are false racist ideas
- in medicine that have persisted in medicine, about differences between races in lung capacity
- of like a person's underlying genetics or genetic risks. And so we ask these large language models these questions.
- Egfr is kidney function. And so like, you, shouldn't, you know, we shouldn't be using race anymore in kidney function.
- assessment. And what we found is that we asked each 1 5 times, because the answers are not always consistent.
- and what we found is that the models all of these models gave answers.
- That we're concerning for almost everything
- And so like, this was actually, this paper got a lot of attention. It got picked up by the Associated Press and covered on like a hundred news outlets. And
- I went on, Npr, if you look up my name and Npr. Science Friday, you'll see that we have a science. We had a science Friday
- program about specifically talking about these research research findings and why they are concerning
- and why we think that you know racist behaviors in large language, models in medicine can cause a lot of harm.
- for the medical professionals using them and then.
- as a follow-up to this study.
- we did something called red teaming, which is a cybersecurity term.
- And that's where we try to identify vulnerabilities in AI models.
- And so it's like.
- Have you guys ever stress tested the models to see like if it'll make them how and when it makes mistakes. Just curious.
- Yes, examples would be great, too, of like what you were trying to do
- if you, if you feel comfortable sharing, it looks like a lot of people. So that's that's like what red teaming is.
- I asked it calculus questions. And so you're trying to see straw. Wait.
- I've once made it say 3 plus 4 equals. 5.
- I gave it the same math questions.
- It always messes up on book. Yeah, it's not reliable for math.
- See this, you guys have done red teaming because
- intentionally say something is right when it's wrong. You would like.
- you would like, we actually wrote a paper about this called sycophantic behaviors of large language models,
- sycophantic meaning that it wants to like please you, and so like, if you tell it that it's wrong, and you like, do it with some authority. It'll like, change its answer.
- Gas lighting it. Okay, you know. Also, models will gaslight humans, too. By the way.
- Okay. So red teaming.
- So we, this was before, like we did this in November 2023. So we just use the models that were available. And we asked it to people to mimic things that you might actually happen in healthcare. So we said, Don't
- don't jailbreak it with prompts, like, pretend you're a racist doctor, because no physician would actually use that prompt.
- And then this is actually a picture from our event. It was like over 80 people, computer scientists, biomedical data, scientists, engineers, physicians. Every table had a physician across multiple specialties.
- Were there inaccuracies, or were there biases where it perpetuated identity based discrimination or false stereotypes. So
- the reason we did this event is actually at many medical systems. Now, we we have access to
- private
- large language models that we can use for medical care questions and things like that. And I actually am very worried about that, because most physicians don't understand how these models work don't understand how they can be when and how they're wrong. Like many of you pointed out, you find that it's not very good at math.
- And somebody said, oh, I found that it's not as good as book summary, so like. Physicians don't know what it's good at or bad at. It's sort of been thrust upon them.
- You know, we're inappropriate, like, not good.
- And we actually released the data set. So this was my like, this is very terrifying. So here's a hallucination misinformation.
- So the question is to calculate the Regisar score for dress, which is a drug reaction. 37 year old lady with a fever of 38.6. Celsius enlarged lymph, node
- atypical lymphocytes. But okay, I know you guys are in high school like this is probably gibberish to you guys.
- this is like a met. This is why, like we were physicians were testing it. They're putting information in
- everything is correct.
- I've highlighted in red.
- What is wrong?
- So the problem here is that sometimes, if you don't know things to deep detail.
- The errors can be very, very difficult to catch, because everything looks right except for one sentence.
- And so I always tell physicians you have to beware of automation bias. So automation bias isn't. And I'm telling you guys, too, is it over reliance on automated Aids. This is a real article. This is a real picture.
- It's another tourist accidentally drove their car into deep water in Hawaii after falling GPS. Please note that this, says another tourist. She was not the 1st one
- that's actually her. I can't believe she's smiling because I would be crying if I drove my car into water like that.
- basically, you know when using GPS people have become so reliant on automation
- that they will override their own good judgment
- and what their eyes see and turn into the ocean
- rather than like their brain. Just doesn't, you know, operate
- at the level to say, oh, I should override what I'm this instruction.
- And so that's that's something you have to be really careful about when you're using these models
- because they make mistakes and they make mistakes in an authoritative manner. And so it can be an issue. I have my rule for using language models in work is that I only actually use them
- in cases where I can certainly find the error. So I actually don't really use large language models to teach me new things, because
- if they're wrong, how will I know if I don't know the answer.
- So you know.
- That's that's something to just be aware of
- and keep into consideration, because the misinformation or the wrong information can be very deeply embedded
- and difficult to assess.
- Okay, I'm gonna talk about vision language models. There's actually different ways that vision language models are trained. There's also something called diffusion models. I'm not going to talk about that today. I'm going to talk about contrastive learning.
- so the way that that works is you take a bunch of images. It's again self supervised, meaning. Nobody has explicitly labeled the data
- and you take in images and the text from textbooks, say, like the captions.
- and you put them into these encoders to turn them into mathematical representations of both the text and the images. And you try to basically
- you try to bring images and text that are paired close together in the representation, and images and text that are not paired need to be further away.
- And so then you can build these models where you put in an image.
- Because I'm a lot of this stuff is about dermatology, because I'm a dermatologist. So we've done a lot of research in this space. So people I've had at conferences. People tell me.
- Yeah, I like to put my like skin images into this ma into these models.
- And so we actually like tested Gpt. As a dermatologist. We even gave it diagnostic choices.
- And what we found is that it was calling stuff skin cancer like all the time. These are the top. 5 true diagnosis, melanocytic nevi, which is just a benign mole.
- is like the top diagnosis, and yet it was mostly predicting like skin cancers.
- So it was like calling everything skin cancer.
- so this is actually that leaves us open for a good portion of discussion which I think would be good. So AI is already entering medicine generative AI has the potential to improve our models by allowing. Oh, we didn't do the auditing so. But
- You know we have to. We have to be very, very careful when we're thinking about how we're using General AI and healthcare.
- This is my team.
- This is actually an older picture. We like to eat a lot.
- So we go out to eat. I'm happy to take questions and have discussion. It can be in the chat.
- or it can be through through
- unmuting and asking the question directly.
- I know you guys who are so active in using the chat box. What have you learned? Being the head of a lab? What have I learned that? I actually already knew this, that the best part about being the head of Lab is that you work with a lot of very smart students with good ideas. And if you empower them, then it basically means that you get a lot of really good work out of it.
- How do I use AI in my everyday life?
- Well, the thing is is that I
- I don't actively use AI date on a daily basis like I will jump to chat, gpt
- if for very, very particular
- tasks. Let's say I I'm trying to think of a good title for a paper, and I put in the current title, or I put in the abstract and say, I need some creative titles. But or I, I don't use it every day. But that doesn't mean AI isn't around me every day.
- because, like I went on to a talk by Netflix.
- The Netflix AI team has a really strong AI team, and not only I learned stuff that I didn't know. So like, you know Netflix makes you recommendations based on what you've watched before turns out they not only make you recommendations, they actually pick
- what like image to represent the show, or what clip to show of the show when you log in, like, you know, to make you watch it. They like engineer. Even the picture that they show you to make you watch it. So that's great, like like, there's just all of us are influenced by AI every day, even if we're not actively using it.
- Okay, I see both hands up and questions.
- okay, I don't know which one which.
- 31:06
	I've kind of been keeping track of who went first, st so we can go back and forth. But Claire raised her hands first.st
- 31:12
	Okay, yeah, okay, let's go ahead and then ask Claire.
- 31:16
	Thank you. So I'm also very interested in like medicine combined with AI. And then I was wondering if you thought it would be like better to study computer science 1st and then apply it to medical problems, or to start with medicine, and then like, learn like the AI side of it.
- 31:32
	Honestly think it would be better to study computer science as an undergrad. I studied bioengineering. This is the whole field was not what it was when I studied undergrad. So
- I didn't know. This is what I was going to get into. So but yeah, I think computer science, there's a lot of now. So I actually just had a medical student who was a Cs computer science undergrad. And then it did an Md. And she came into my lab and did research for a year. And she was spectacular like it was great. It worked out really well. I also have a student who did a math who did an Md.
- He did like he did some kind of chemical engineering. Then he did his Md. Then he did master. So there's a lot of different ways. I'm not saying there's 1 way to do it. But if you like, computer science, go ahead and do the computer science
- and then go to medical school.
- 32:22
	Okay. Thank you.
- 32:24
	And the next was Teresa.
- 32:28
	Yes, it's pretty nice to meet you. And our mentor is Leslie. And we discussed about a few medical questions during our discussion. And actually, yeah, we had a question like, we're kind of curious. How do hospitals like account for the storage? For like such amount of data like, there's ehr? There's also audios. So we're kind of curious like, what's the answer of this question?
- 32:56
	Oh, yeah, I mean their Ehrs. I mean they have.
- There is a whole informatics team at hospitals. There's a chief informatics officer, so that the whole way that it's stored I'm not like privy to. But basically, you know, for images, there's there's a whole system for radiology images called
- and then there's a whole bucket that stores like dermatology images. I recently learned that videos of colonoscopies actually get deleted because they're so large. There's no way you can hold on to them.
- So there's just all these very hot. It's a little hodgepodge.
- 33:42
	Thank you very much.
- 33:44
	And then we had another question in the chat. Do you think from this is from Alex? Do you think that accessibility to more advanced AI models is an issue in healthcare.
- 33:54
	Alex, could you clarify that question? I want to make sure I understand it correctly.
- 33:59
	Like. What I meant was, do you think like that the issues about like not being able to access the more advanced models that like could be used to like, give more accurate diagnoses.
- 34:10
	Oh, I mean, internally like, when we did our study we were just using Gpt for. But now, like we have access to all of sort of like, you know. 3 0, 4. 0,
- well, we are. We are getting access to more advanced models
- when we test stuff like my team is running a test on a bunch of stuff. Right now again, we're like testing deep seek. We're testing all of these models to see how they do.
- 34:41
	Oh, okay. Thanks.
- 34:43
	And then, Max, you had your question next.
- 34:46
- 35:00
- facts. So like I taught my nurse how to use it to write letters to insurance companies, where I tell her, give it the bullet points of why we need to give this medicine, ask it to like. Generate the letter. I'm a little bit wary, like what somebody mentioned about book summarization to ask it to summarize patient information. Because what if, unless I already know all the patient information, because what if it misses something? And I didn't know that, or like
- I also think we have to be really like, for example, asking medicate, like people have done studies looking at medication doses. It gets medication doses wrong. So like, you know, there are things where it's like it could really cause a lot of harm.
- 35:48
	Okay. Thank you.
- 35:50
	And then there's another message in the chat from Arsha, saying that they remember a slide with the data of which models.
- 35:57
	Oh, okay.
- 35:58
	Medical issues with Llms. Most. So if somebody were to use a chat with a model about medical issues, which one would you suggest.
- 36:07
	None. If you have a medical issue, talk to a doctor.
- 36:14
	And then the next question from Erin.
- 36:20
	Oh!
- 36:20
- 36:39
	It depends on what you're trying to train, because a lot of people are trying to train things that no medical knowledge which doesn't have any protected health information. So like they're trying to use
- retrieval, augmented generation where they're like having, you know, a body of text that they think is accurate. That's different than like models built on electronic health record data which people are doing as well.
- And usually the way that's done is by Re by teams within the hospital system who have authorization either. You know, a lot of hospitals also work to remove identifying information. There's actually a list of what's considered identifiable information in patient data. And so they strip it of that and then use the data that's left over for training things.
- 37:33
	Great, and the next question is from Alexander.
- 37:36
	Hi, doctor, I mean Professor Denisu, it's nice to meet you. My name is Alexander, and I wanted to ask in your work in at the intersection of AI and medicine, like which form of AI like computer vision, national.
- like Nlp or deep learning, do you? Do you use most often.
- 37:56
	Our lab actually works with all of the above.
- yeah, because we have a group. You know, the same way, you guys are broken up into different groups. There's groups of people who are focused on computer vision. There's groups of people who are really interested in large language models. And there's groups of people who are interested in multimodality.
- Sometimes some groups focus only on one, but we have teams in all.
- Somebody asked me, do you believe medical AI tools should be regulated like drugs or medical devices. Yes, actually, the FDA does regulate right now. It's a little bit wild, Wild West, because they're trying to catch up with the field that's moving very fast.
- And so we've written on. FDA authorized AI medical devices. Actually, I was just here. If I give a stat hold on. I was just in a news article quoted, I think, in a news article I don't know if she actually quote quoted me.
- My computer doesn't acta.
- Go to everybody.
- Okay? And then Luna asked in a direct message, what challenges have you faced when working across such disciplines that seem so different at first? st Look AI and dermatology. And how do you ensure both sides? Understand the ethical stakes? That's a really good
- question. Very mature question for someone who probably did doesn't realize, or you do realize clearly sorry. You clearly realize that interdisciplinary work is very hard. Because
- for the longest time people in dermatology didn't think AI had anything to do with them, and was like this weird niche field. But over the last 2 years they've come to recognize. Oh, this is really penetrating medicine, and I think on the AI side
- engineers don't always understand the nuances or potentials for harms that patients might face with AI. So I spend a lot of time
- talking to people on both sides and tailoring presentations to those specific audiences to try to teach them what I think they need to know.
- 40:42
	Great. There's a question in the chat and a hand raised from Suman. You can go ahead and ask it directly.
- 40:49
	Oh, yeah, my question in the chat was already answered, but I have a different question, since we don't fully understand what causes. Like many cancers. Do you think that unsupervised machine learning could help us find, like hidden patterns in patient data that might point us towards these causes?
- 41:05
	Yes, and that's the whole idea between.
- we're doing a large multimodal study right now on lung cancer where we're bringing in the radiology, imaging the pathology, the molecular data, ehr data to see if we can find any kind of predictive patterns in there.
- 41:23
	And next is Sammy.
- 41:27
	Hi! I really like your talk and I like how you like highlighted the blind spots of like large language models as well and stuff
- you know, like Geoffrey like Hinton. I don't know if I said his name, but like he said something about
- like in like 2016 about how you know.
- AI was gonna like outperform a lot of like radiologists in 5 years. But, like, you know, clearly, they're still here. I wanted to kind of know, like where you think the job market and medical AI is like moving towards, you know, with like the demand and stuff, and like where the jobs are gonna be are kind of gonna be. And I guess that field, especially since you just highlighted like all the mistakes that they do make.
- 42:09
	Yeah. So radiology has a shortfall now. And I think part of it is because people got scared. So not as many people went into it. And now we don't have enough radiologists. And it turns out that, you know.
- and there's a lot of radiology, AI tools that provide assistance. But
- you know, it's there. Some workflows may be automated, but there's always gonna be a need for a human to do something. It just how the human works might change. So let me give you an example.
- When Ct scans and MRI and Imaging came.
- Now they used to be medical students were really good at things like using the stethoscope which we still use it. Percussion where we like tap the abdomen to feel fluid. Nobody does. Nobody relies on that anymore. They're like, oh, why would I have to use a physical exam to tap up where the fluid is? I can get imaging
- right? So it's not going to. It's it. Things will just morph with the technology. I was actually having dinner with a friend who's a Gi doctor and she uses AI.
- She has an AI tool when she does. Colonoscopies that help point out where the Polyps are for her to, you know, cut. And so she has this like backup sort of tool that's aiding her. So she still has a job, you know. So
- I really don't. I think it's very hard to predict. But what will basically happen is there will just be shifts. And how in how and what humans do within those specialties. I mean, there's also medical specialties that didn't even exist before, like palliative care has become bigger, which is all about the human touch and helping people at end of life. So it's like it's like. I wouldn't worry that there's like going to be a job loss in medicine.
- 44:04
	And next we'll hear from Audrey.
- 44:09
	Hi! I'm Audrey. And I was just wondering as someone who's like, really like in the actual, like hospital setting kind of. And in the field. Do you think I mean, it's kind of similar to Sammy's question. But do you think predictive AI models, especially for like diagnostics, will be able to gain enough clinician trust to actually be used in hospital settings, at least, maybe in the near future.
- 44:35
	I mean, yeah, I think that's a good question. I think that some things are already being used. So generative. AI and traditional computer vision. I mean, computer vision has its issues, too. But it's like much more stable than, say, generative AI like, there are a lot of computer vision. Algorithms have been. FDA cleared, and, as I mentioned, one of my colleagues was saying, she uses it when she like, does her colonoscopies. So you know
- there are things that have already come through into the medical system.
- I just think we're not at a point where, like the doctors should fully trust the chat bots.
- 45:13
	Okay. Cool. Thank you.
- 45:16
	There's a question in chat from Arsha. What's the most surprising or unexpected result you found from your research, either technically or ethically.
- 45:24
	Same.
- That's a good question.
- I think we did this one study where we looked at humans using AI. So first, st we looked at humans.
- and we found that they were biased in diagnosing skin disease across skin tones which wasn't surprising to me.
- because the textbooks are very skewed towards white skin. But what surprised me is, we gave them a fair AI model, expecting them to use that AI model to improve in performance, and they used that model differently across skin tones, meaning that they seem to trust the model in one setting more than another, and that was kind of surprising to me. So I think
- my like punchline is always like, if you're gonna look at how AI works.
- You really need to understand how not just how the model works. But how? What the impact is when it interacts with the human.
- 46:27
	Great, and Anastasia, I see your hand raised.
- 46:31
	Hi, I'm Anastasia. Sorry my video is off my Internet's kind of bad. So my question is that I've heard sometimes family doctors can get diagnosis wrong, especially because they have to memorize so many of the diseases and diagnose you on the spot. And sometimes, like people say that AI is just as good as its data. So do you think the data can ever be so good to a point that the AI model can be more accurate than some family doctors.
- 47:02
	It depends on how you're testing. So problem is that like a lot of people who have run these tests have looked at what we call a clinical vignette, where it's like it's like a perfectly package. Your patient comes in with these symptoms, and you know they come in with a sore throat and a run like. But that's not actually how medical practice works. Doctors obviously have inaccuracies as well.
- I think the question is, how do we get to a point where Doctor plus AI is better than doctor alone, and we haven't had a we haven't really gotten to that.
- To that situation yet.
- 47:46
	Thank you.
- 47:50
	And we have time for a couple more questions here. There is
- another one in the chat from Hannah, other than medicine. What other fields will AI augment and have the most influence in.
- 48:06
	Oh, hmm!
- I mean, it's having a lot of influence in radiology. But, like the radiologists have like, see? Like they've seized the moment they're not like, they're like actually building models. They're actually taking the lead. And so it hasn't been like Jeffrey Hinton said. They haven't been replaced.
- They've just yeah, they've just sort of taken the lead on it.
- 48:35
	And then, Bessie, I see your hand raised.
- 48:37
	Hi. So we were kind of talking about this in our little like medical AI group as well. But like as medical AI continues to advance, and the data gaps between like the developed countries and the developing countries widen. How do you think we can ensure that data collected from the under-resourced nations are sufficient, especially when many of those people in those regions
- lack like basic access to health care and proper health records. And should we prioritize this development? Or should we focus more on like the quality and dedicating those resources
- to reducing the bias and including the underrepresented populations.
- 49:16
	I think that we should apply resources to giving that people in under resource settings, real public health care. And you know, people are saying, Oh, yeah, AI will solve all these problems. And I'm like, Well.
- you know, that's not right. Because why is it fair that they just get like an AI that doesn't work so well like they need. We need to get good public health infrastructure everywhere, and also try to use AI in ways that we can to improve like access to care. I also do think we need data from globally
- in order, especially if you're going to apply like, like local data is most important. So for I'll give you an example like, let's just stick with the Us. Okay, and you're building a skin disease Classifier. If you build a skin disease identifier, I'm gonna call classifier
- only from data from California where I am.
- you will not have a lot of images of Lyme disease.
- which has a very particular appearing rash.
- The east coast has a lot of Lyme disease, so if you build it on the east coast. You'll have pictures of that. But you wouldn't have that. And so that's why it's really important that the data used to train models
- be representative of the area that that model is going to be deployed in. I hope that answers your question.
- 50:42
	Yeah, thank, you.
- 50:47
	Okay? And then there's, I think, 2 more in the chat. The 1st that I'm seeing is from Sanakshi. How do you think we can rectify the mistakes or biases in AI, especially in the medical field, with a few others commenting on that question as well.
- 51:04
	So there are metrics of fairness. Somebody said that.
- But actually, none of those techniques necessary, like, for example.
- Okay. So we actually wrote a paper even on synthetic data generation techniques and showed that synthetic data is not as good as real data. So you still have a bias. If you're using synthetic data to represent an underrepresented group compared to the overrepresented group. So I say, there's a couple of things one you need to actually like, do the appropriate data collection, so that you have representation of all the groups
- and 2 like, if we're talking about language models, like, if we're talking about image-based bottles like getting more images of the thing that you're missing is like the most
- appropriate way to deal with it. If we're talking about language models like when I showed racist behaviors of language models. There are ways to use reinforcement learning to essentially like.
- make it realize that a lot of that maybe the text that it was pulling from is inaccurate and biased.
- 52:11
	Thank you.
- Thank you.
- 52:15
	Great, and it looks like the last question was answered, oh, there's 1 final question in the chat, Dr. Donishu, if that's okay to answer. Does the black box of deep learning models obscure biases in medical AI. And how should we address this issue?
- 52:31
	Yeah, it can one. You have to look at your outcomes. So you have to look at your outcomes against across groups, vulnerable groups. So, even if you have a black box, you can look at whether it's accuracy, you know. Is equivalent across groups. There's many different fairness metrics, as was mentioned.
- But also, you know, we've worked on explainable AI to like pinpoint which of those factors are causing
- sort of biased behaviors like trying to actually understand how the AI model makes the decision that it makes. And so that's like another way to try to understand that.
- 53:15
	Fantastic.
- Dr. Danishu. Thank you so much for your time, and just spending so much time answering questions you were able to get to. You were the 1st professor to get to all of the questions in in the chat. So you you win the award for being the most effective, to address all questions from our students.
- 53:37
	No problem.
- 53:39
	Thank you so much for your time and your expertise. We are recording this session. So we will be sharing the recording out with students, and if you are open to sharing any slides or any additional information like the links you can send that our way to, and we'll get that out to to them. But thank you for your time.
- 53:58
	Yeah. No. Problem. Alright take care. Guys.
- 54:01
	Thank you.
- I'm gonna go ahead and.