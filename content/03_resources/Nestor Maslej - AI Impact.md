---
title: "Video Conferencing, Web Conferencing, Webinars, Screen Sharing"
source: "https://stanford.zoom.us/rec/play/14nltL07d8Shvu9mr5wY0Hm9FTfnzLv5tNVPcM5xx3sVeoA7XZmA1GLsQddVSePeNx11Jqs5oexYeM26.028osfHpzlMfNfA8?eagerLoadZvaPages=&accessLevel=meeting&canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fstanford.zoom.us%2Frec%2Fshare%2FN9S_YuDcgxzAEAjqVCEfvQ7Kxv0VotYPcXDc3qwpGTBIWexwzcDEyb_EUm2lZXKO.503TVLQMNK1QSgPZ"
author:
  - "[[Zoom]]"
published:
created: 2025-07-04
description: "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference room solution used around the world in board, conference, huddle, and training rooms, as well as executive offices and classrooms. Founded in 2011, Zoom helps businesses and organizations bring their teams together in a frictionless environment to get more done. Zoom is a publicly traded company headquartered in San Jose, CA."
tags:
  - "clippings"
---
## Summary
Nestor Maslej, a research manager at Stanford's Institute for Human-Centered Artificial Intelligence, presented key findings from the AI Index, an annual report tracking AI ecosystem developments since 2017. He highlighted that AI performance continues to improve rapidly across various domains like image and video generation, reasoning, and scientific applications. AI is increasingly embedded in daily life (e.g., medical devices, self-driving cars) and businesses are significantly investing in and adopting AI, driven by demonstrated productivity gains, especially for lower-skilled workers. Governments worldwide are increasing AI-related regulations and investments. Public opinion shows rising optimism but also persistent regional divides and distrust (e.g., self-driving cars), with broad support for regulation despite disagreements on its type. Geopolitically, the US leads in top AI models and influential research, but China is rapidly closing the capability gap and leads in research publications and patents. The responsible AI ecosystem is evolving unevenly, with more reported incidents but a lack of standardization in safety benchmarks, and businesses are still early in their responsible AI journeys. While AI is becoming more accessible and cheaper for general use, frontier intelligence remains expensive, and industry continues to dominate AI development due to high training costs. Challenges remain in AI education and complex reasoning tasks.

## Key Points
- **AI Performance Improving:** Rapid advancements in image/video generation (e.g., Midjourney, Will Smith videos), reasoning (test-time compute), and scientific applications (Nobel Prizes for AI research).
- **Increased AI Integration:** More FDA-approved AI medical devices and rising adoption of self-driving cars (e.g., Waymo) with improved safety records.
- **Business Adoption & Investment:** Significant increase in AI usage across organizations (80% in 2024, up from 20% in 2017) and a surge in generative AI private investment (from ~$4-5B in 2022 to ~$34B in 2024).
- **Productivity Gains:** AI is linked to improved productivity for customer support, scientific discovery, and tasks in computer science, consulting, and law, with a disproportionately positive impact on low-skilled workers.
- **Government Action:** A sharp rise in AI-related regulations in the US (from 1 in 2016 to 16 in 2024) and globally, alongside billions in government infrastructure investments.
- **Public Opinion:** Growing global optimism about AI's benefits, but deep regional divides persist, and distrust (e.g., in self-driving cars) remains. Strong bipartisan support for AI regulation among US policymakers.
- **Geopolitical Intensification:** The US leads in producing top AI models and highly cited research, but China is rapidly narrowing the performance gap and leads in AI research publications and patents.
- **Responsible AI Evolution:** An increase in reported AI incidents (ethical misuse, deepfakes), but a significant lack of standardization in responsible AI benchmarks compared to general capability benchmarks. Businesses are still in early stages of mitigating AI risks.
- **AI Accessibility vs. Industry Dominance:** AI models are becoming cheaper to use and smaller, with better performing open-weight models emerging. However, industry continues to lead AI development, primarily due to the exponentially high costs of training frontier models ($40M-$200M in the last year).
- **Education Gaps:** While post-secondary computer science graduates are increasing, K-12 teachers feel ill-equipped to teach AI.
- **Remaining Challenges:** AI still struggles with complex reasoning and planning tasks, suggesting new architectural approaches beyond mere scaling may be needed.
- **Future Outlook:** Continued technological improvement is expected, but the broader impact hinges on how well society learns to apply and integrate AI, emphasizing the need for thoughtful regulation and public trust.

---

https://stanford.zoom.us/rec/play/14nltL07d8Shvu9mr5wY0Hm9FTfnzLv5tNVPcM5xx3sVeoA7XZmA1GLsQddVSePeNx11Jqs5oexYeM26.028osfHpzlMfNfA8?eagerLoadZvaPages=&accessLevel=meeting&canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fstanford.zoom.us%2Frec%2Fshare%2FN9S_YuDcgxzAEAjqVCEfvQ7Kxv0VotYPcXDc3qwpGTBIWexwzcDEyb_EUm2lZXKO.503TVLQMNK1QSgPZ
# Original Content

[Accessibility Overview](https://stanford.zoom.us/en/accessibility)

<video src="https://ssrweb.zoom.us/replay02/2025/06/24/DC0159E2-5E5B-4CB0-B283-98BBA705E90E/GMT20250624-195303_Recording_1920x1120.mp4?response-content-type=video%2Fmp4&amp;response-cache-control=max-age%3D0%2Cs-maxage%3D86400&amp;data=ba89b7a4df6f0b14f6c49cce8fafe06ecf7a69d8d0c89f91def92d21f26953e0&amp;s001=yes&amp;cid=aw1&amp;fid=-sZS7McPfV2TxianufL8mDogx9NHatJ05i2XwQXrqWpgFtWL82j0l9wTj1mfy0HKGYo5uwZWzxrfqV6H.TxBDUqzB3fywlonq&amp;s002=6GlAJqU7WfXp-0rbqnbDByDAix5rK_DGzctoalwcADnpEuMkgTWA9sG_QYvA22RIxe9SWPIQBLimaj-PZH08UoWNJHP7.Y0L7GZOGJXPJB6Da&amp;tid=v=2.0;clid=aw1;rid=WEB_12c94f0a349866022860c78608cde5d4&amp;Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6Imh0dHBzOi8vc3Nyd2ViLnpvb20udXMvcmVwbGF5MDIvMjAyNS8wNi8yNC9EQzAxNTlFMi01RTVCLTRDQjAtQjI4My05OEJCQTcwNUU5MEUvR01UMjAyNTA2MjQtMTk1MzAzX1JlY29yZGluZ18xOTIweDExMjAubXA0P3Jlc3BvbnNlLWNvbnRlbnQtdHlwZT12aWRlbyUyRm1wNCZyZXNwb25zZS1jYWNoZS1jb250cm9sPW1heC1hZ2UlM0QwJTJDcy1tYXhhZ2UlM0Q4NjQwMCZkYXRhPWJhODliN2E0ZGY2ZjBiMTRmNmM0OWNjZThmYWZlMDZlY2Y3YTY5ZDhkMGM4OWY5MWRlZjkyZDIxZjI2OTUzZTAmczAwMT15ZXMmY2lkPWF3MSZmaWQ9LXNaUzdNY1BmVjJUeGlhbnVmTDhtRG9neDlOSGF0SjA1aTJYd1FYcnFXcGdGdFdMODJqMGw5d1RqMW1meTBIS0dZbzV1d1pXenhyZnFWNkguVHhCRFVxekIzZnl3bG9ucSZzMDAyPTZHbEFKcVU3V2ZYcC0wcmJxbmJEQnlEQWl4NXJLX0RHemN0b2Fsd2NBRG5wRXVNa2dUV0E5c0dfUVl2QTIyUkl4ZTlTV1BJUUJMaW1hai1QWkgwOFVvV05KSFA3LlkwTDdHWk9HSlhQSkI2RGEmdGlkPXY9Mi4wO2NsaWQ9YXcxO3JpZD1XRUJfMTJjOTRmMGEzNDk4NjYwMjI4NjBjNzg2MDhjZGU1ZDQiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTE2NDM5MDl9fX1dfQ__&amp;Signature=GWnJ6J71JERfaVkCoZ5a0refs-DeaJsUIN5N5Z8QmvzHut8Ca6r7fFmRblLYWMv1k5ienJtLJIYL~qpWJEHs0orOH4XMKuf9cAyKBH9Cm4uxZsUBxMZ-Wi~CGGRyzfnEozv1BM2UoK-R5VRaWM5kf3NA9Q1e3NhESkNchy~6qJQtJelkAAsKrS8b8SC1JmVnxS~lPBYMGF0ZBBQhZBw1~5JnDIwCrQ-iMXWHTbxZPTVMuLMb0IH72q8z4v08GbY3BTKBloT4vChrG6qglO2cG856GvWdf3UuxBldqKqzy9TuyN3Umul30U2Oa2ioCdW0gq3l0oWfwniX0F-gQh3OSQ__&amp;Key-Pair-Id=APKAJFHNSLHYCGFYQGIA"></video>

00:00:00 / 01:01:56

## Audio Transcript

## Chat Messages

- 00:01:44
	Alright. Hello, everybody.
- Okay, we're going to go ahead and get started. I'm going to welcome our speaker for the day.
- Nestor Maslick. He's on the call with us. Now everybody welcome. Nestor Nestor is a research manager at Stanford's Institute for Human Centered Artificial Intelligence, where he oversees the AI Index
- and the global AI vibrancy tool. His work aims to make AI advancements more accessible to policymakers, and the public.
- Nestor has contributed to global discussions on AI providing insights that have informed policymakers in the Us. Canada, Germany, the Uk. China, Japan, and Korea.
- He regularly briefs high level officials, testifies before national parliaments and presents to industry leaders. Additionally, he is a fellow at the center for international governance innovation. Cigi, where he writes about AI developments.
- 00:03:09
	Great awesome to be here real pleasure to present to this group.
- you know. Fancy introduction for me. Thank you, Miriam, but I'm really just a guy that makes a lot of AI charts. And I'm excited to walk you guys through some of them today.
- Now, I'll kind of start this presentation off by saying that
- you know my day job is to kind of manage the AI index. It's this annual report that looks at what's happened in the AI ecosystem. And what's happened in the AI space.
- And this report was created
- for the 1st time in 2017 by a diverse committee of AI thought leaders.
- people like Jack Clark, who's now one of the co-founders of anthropic, arguably opening eyes. Biggest rival
- and Eric bring Olson, who, I would say, is one of the world's leading digital economists. And these individuals got together in 2017, because they understood as far back as then that AI was going to be a game changing technology, and that as such it was going to be valuable to have some kind of report that could really tangibly tell a story of
- what is really happening with this text. So how much it's getting better, how policymakers are reacting to it, how businesses across the world are reacting to it. And that's really what the AI index is aimed to do. Now we collaborate with a lot of students and contractors in and outside of Stanford, we
- work with data vendors across the world
- that provide us data that we include in the report, and we've really been privileged over the years have been cited in newspapers across the globe to have been cited by governments and government agencies across the globe, and increasingly also briefing business leaders across the globe.
- And I think that this 3 part structure is really reflective of the fact that AI is now something that is being talked about in parliaments across the world.
- It's something that's being talked about in boardrooms across the world, and it's something frankly that's being talked about. Kitchen tables across the world, you know. So many different stakeholders have a seat at the table and something to say about the technology.
- And that's why it's important to speak to audiences like yourselves. Right? You know, you're the next generation of leaders. The next generation of people that are going to be thinking about this tech and what it could do. And it's important that you guys can understand what it can do, what it cannot do, what some of the policy implications are, what some of the economic implications are. So I'm very happy to speak with you guys today. And I think in a lot of ways the mission of the AI index is to serve people like yourselves to
- better understand what's happening with this technology. Now let's talk a little bit about some of the top takeaways. I mean the AI Index report itself is close to 500 pages long, and I mean, I'm sure most of you guys are either on your summer break now, or perhaps getting there, so I don't want to start the summer break off by assigning 500 pages of reading, so we'll just walk you through some of these top takeaways to make your lives a little bit easier.
- Now, the 1st takeaway is simply that AI performance is continuing to improve. Now, I think one of the ways you can see this really well is that if you go to mid journey. And, for example, you look at a hyper, realistic image of Harry Potter. This is what we asked each version of mid journey from
- the 1st one that came out in February of 2022 all the way to one of the more recent ones that came out in July of 2024, you can see just how much better these generations have gotten. Now, I think this slide really kind of puts into perspective
- how much AI has improved, and how little time. Now, frankly, if I actually had to hang one of these in my living room, I'd probably go with the one on the left. It looks like Pablo Picasso has drawn me an abstract cubist, Harry Potter, who's tortured. But of course, you know, we asked for hyper realism, and that's what we get with the image on the right.
- you know. You can also do some fun exercises where you go to websites like.
- which face is real.com. And these websites are going to give you an image of a real human face. And then an image of an artificially generated one. And you're basically going to be asked to identify which one you think is real. And in this case it's the one on the left that is, in fact, real.
- You can also see some of this tremendous progress occurring with videos as well. One of the things that we did at the Index is we went to some of the leading
- AI video generators like Peek up. And we asked them to generate a video of Will Smith eating Pasta. Now, this is what you get from that in December of 2023. Not super great right? The guy kind of looks like Will Smith.
- Now, if you fast forward and you go to October of 2024,
- now, flip the script a few months further. Now you're kind of at a year and a half out from this point, and you see that there's been tremendous progress even further. Right? So this is the best video that we've had yet again, not perfect. The hands are a bit funny. The fork is a little bit strange, but the pasta is no longer levitating into this guy's mouth. He looks very happy as he's munching on some spaghetti, and this is definitely the best quality video that we've seen here so far.
- And you see that AI has progressed in a lot of other domains as well. It's not just image and video. You have a lot of new reasoning paradigms that have emerged like test time compute. So test time. Compute is one of the big developments that occurred in 2024. And it refers to these kinds of AI systems that
- iteratively reason. So these systems kind of reason, as you answer them questions. And in the process of doing this iterative reasoning, they're able to do a lot better on benchmarks than previous systems that existed. So they especially do a lot better on benchmarks like Math and Amy, which are looking at some of these reasoning challenges.
- In 2024, one of the big challenges with AI was that it started to be used a lot more in science. And this was a challenge, but an opportunity as well. And you saw the impact that AI had in science by the fact that certain scientific developments like Alpha Fold 3 were now kind of awarded the Nobel Prize in chemistry. You had other AI research that was
- awarded the Nobel Prize in physics. So AI started getting a lot of scientific recognition for what it could do on a science perspective.
- Another AI buzzword that we saw in the last year was AI agents. So AI agents are these kind of semi autonomous systems that can operate flexibly within different environments. And you'll notice that on some of these benchmarks like rebench, these are benchmarks that are basically trying to measure how well these systems
- do a variety of these autonomous tasks.
- When these AI systems, these agents are given very low time budgets. So that is kind of not a lot of time to do a task.
- You'll notice that they tend to outperform humans.
- But when you extend the time budget, that is, you give humans and these agents more time to do things the humans tend to do massively better.
- And I think that's reflective of the fact that humans really outperform a lot of these AI systems when the tax become more complex and multi dimensional.
- Now, where does that leave us? Well, this is a chart from last year's index, and we can say that on a lot of benchmarks of artificial intelligence performance AI is kind of already exceeding the performance of humans. I mean, this was last year, and you'll notice that even last year there was maybe a few benchmarks still, where these systems weren't quite at the level of humans. But if you fast forward this year, you know, we could really kind of ask ourselves what benchmarks are left, what challenges are left where these
- AI systems are behind humans. And I think the story here is not only one of
- AI systems beating humans on a lot of these benchmarks, it's really the pace with which you've seen these massive developments, you know. For example, if you look at the benchmark on the left, that's the benchmark for Imagenet. It basically took the AI Research community.
- I would say a good 4 years
- to go from models that performed 90% relative to humans on that baseline to exceeding that baseline and keep in mind that Imagenet is a benchmark, for how well these systems can classify images. It's a relatively straightforward task.
- Conversely, if you look at some of the benchmarks on the right, like the line in the teal. This is a benchmark for competition level mathematics. And we basically got an AI systems that have scored roughly, 10% relative to the human baseline to exceeding that baseline in 2 and a half years. And for reference.
- the human reference level for that benchmark is the performance of a 3 time international math, Olympiad, gold medalist. So it's not like they're testing these systems against me. I still have nightmares from grade 12 ap calculus. They're testing these systems against someone that actually knows how to do math.
- And I think the takeaway for your audience should be that AI is getting better. It's getting stronger, and we're really kind of at a moment where
- I think we have to ask ourselves what challenges are left.
- and some of the new benchmarks that are being proposed. I mean, literally, they've been labeled as Humanity's last exam, the kind of premise almost being that there's going to be very few intellectual tasks left on which we're going to want to test these systems and see
- these systems on.
- And even with this kind of benchmark humanities. Last. Exam, just a quick thing to kind of point out, when we publish the AI index, the top performing model
- got a score of roughly 9%. And within a month or so the next best system. O 3, it kind of bumped its accuracy to 20%. So even on some of these more challenging systems. The new models are getting stronger and stronger
- on a broader level. I think there is an interesting kind of, you know, exercise and kind of zooming out here and kind of wondering, where do we actually go with these benchmarks? Right? Because
- benchmarks have been a historic way of evaluating some of these AI systems, but they're not perfectly designed. They suffer from issues like contamination. They don't always kind of well represent what they're trying to measure.
- And I think we need to kind of wonder
- what else we could do to potentially
- think of how we can measure these systems and monitor these systems in kind of intelligent ways.
- Now, as AI has improved technologically, it's also become increasingly embedded in everyday life, and that kind of point directly flows from the first.st Right. Better technology means more usage.
- Self driving is also on the rise. If you look at several cities in the United States, like La San Francisco, Phoenix, and Austin, there have been waymos that have been driven on these cities. Now for a while.
- If you look at data like the chart in the bottom left.
- you'll see that on a variety of benchmarks. These waymo drivers are a lot safer than humans. There's less airbag deployments, less injury reports, less police reports. And this phenomenon of self driving isn't something that's just occurring in the
- Us. It's also occurring in places like China. Companies like Baidu have really started to pioneer the launch of artificial intelligence driving on their streets in China.
- So better technology has meant more AI usage in day-to-day life.
- to kind of hovering around 50% in the last few years, and then jumping suddenly all the way to 80% in 2024. You can call this the Chat Gpt effect chat Gpt comes out in late 2022. And suddenly organizations report using the technology a lot more
- generative. AI investment is also on the rise. So if you look at global private investment in generative AI, you go from roughly 4 to 5 billion of this investment in 2022, all the way to 24,
- in 2024, having close to 34 billion.
- And there are certain geographic regions like the United States that are really kind of led in the total amounts of this investment. So Europe, China, much further behind the United States when it comes to growing and scaling this investment activity
- and part of the reasons that I think businesses have become increasingly kind of invested in this technology is because there's more and more research. Now that highlights the positive productivity impacts that AI has.
- So if you look at AI, for example, its impact on customer support agents, you'll see that there are studies that show that people that use AI are able to answer a lot more hourly chats than those that don't.
- If you look at AI and scientific studies, you'll find that AI also is associated with better scientific discovery.
- There's also studies that have come from computer science that show that
- people that use AI, they are able to get a lot of tasks done in substantially less time compared to those that are not using AI tools. And if you still don't believe me, you can kind of look at research that comes out of consulting and law that show that consultants and lawyers that use AI are much more, let's say, productive. They get work done quicker and submit work of substantially higher quality.
- A lot of this productivity literature also seems to suggest that AI has some differential impacts across skill groups.
- So a lot of these studies, they disaggregated the impact that AI had on low versus high skilled workers, and all of these studies found that it was low, skilled workers that really benefited a lot more, that they tended to have their performance increase by a lot greater amounts than the high skilled workers, and this kind of finding has a lot of important implications for businesses that are interested in using AI to level their operations.
- Now, this leads to kind of an interesting, existential question, which is really, what does the future hold for the labor force? We don't have a lot of good data on whether AI as a technology is going to kind of augment or automate jobs.
- But we can look at data on the expectation that workers have about the impact of AI on organizational workforces.
- And you'll notice that most managers
- are a little bit more pessimistic. They're predicting that AI is going to decrease the number of employees within organizations. But you'll notice that the amount that are predicting decreases has actually gone down. So less and less are predicting decreases, which I think is kind of indicative of the fact that
- people maybe are starting to use AI and are finding that, you know, while it is saving them time, they can in a lot of cases, reinvest that time to kind of do other things that you know. Maybe historically, they would not have been doing
- you know, it's not just the case that one or 2 regulatory agencies are thinking about this. It's really regulatory agencies across the board that are getting involved in this race. So if you're a business that's working with AI, you do really need to think about how different organizations might be regulating it.
- there are certain states here, like California, Utah, Virginia, Maryland that have really been leading the charge, and have passed the most laws and states as a whole, actually have been pretty good on passing laws about how AI could be used in deepfakes for elections, as well as how AI could be used in deepfakes for intimate injury.
- Now, I mentioned how AI is a topic that's being discussed in kitchen tables across the the the world.
- What does public opinion actually say about how people feel about this technology? Well, public opinion says that even though global AI optimism is rising, there are still some fairly deep regional divides that remain so. This chart is looking at the question, do you agree that products and services using AI have more benefits than drawbacks? And it looks at how people have responded to that question by country. And you can kind of basically see
- how the answers have varied from 2022 to 2024.
- Now for most countries in the sample, there's much more kind of AI optimism now than there was before. So more countries are bullish on AI, especially some of the countries that are the most Bullish are countries like France, Germany, and Great Britain, that are historically very negative about the technology.
- Now, still, despite that
- it is countries like China, Indonesia, and Saudi Arabia that are just much more optimistic. Nevertheless. So I think we have to wonder, when we look at this, what determines?
- Why, some countries are more optimistic than others. And how is this optimism going to change over time?
- And the thing with some of this optimism is that you can kind of say that you know the fears around the technology they persist to kind of despite data that might tell us that we should be thinking otherwise. So, for example, in the United States, if you look at attitudes that people have towards self driving people in the United States remain very distrustful of self driving cars. This is despite the fact that
- self-driving cars are a lot safer than actual vehicles.
- Moreover, there is also broad support for AI regulation among local Us. Policymakers. So if you look at kind of local Us. Officials and the support they have for government regulation by party year.
- we're now at a moment, we're close to 74% of government officials want to see more AI regulation. That's pretty massive amounts. And we see these volumes of
- demand for AI regulation both among the Democrats and the Republicans. Now, it's 1 thing to want regulation. I think that's pretty clear in the data. It's another thing to actually agree on what the right regulation type is the devil's kind of really in the details here
- there are some lawmakers that want. For example.
- more AI reskilling and AI training. And there is other lawmakers that want things like Ubi as a means of accelerating the way in which we could use AI tools. So there's different ways to kind of be thinking about what can be done and what policy approaches are best.
- You also can't talk about AI these days without mentioning the kind of geopolitical race. So the geopolitics of AI has also intensified.
- and
- if you look at the last 2 decades, there have been more models coming from China and the Us.
- And then a couple of other countries like Canada and some other European nations. Rounding this out.
- Now with that, said China, is really closing the performance gap. So if you look at the difference in the performance of the top, Us. Versus Chinese models on a benchmark like Lm. Sis.
- China is also leading when it comes to AI patterns. And this data really kind of illustrates that we're now at a moment in the ecosystem where the stakes are pretty high with AI, and there's different kind of geopolitical perspectives about what kind of things need to be done with this technology.
- Now point number 7 is the responsible. AI ecosystem is evolving, but it's evolving in a rather uneven way. Now, 1st and foremost, I think we want to kind of acknowledge that there's more reported AI incidents than ever before. So by AI incidents, we're kind of referring to incidents where there might be kind of ethical misuse of the technology. So, for example.
- this database that we look at here for the AI index. It's counted as an instant incident. Occasions where Tesla vehicles have
- driven improperly with their self driving function. It's counted as incidents.
- examples where AI chat bots like character, have encouraged teenagers to commit suicide. It's counting as incidents examples where
- deep fakes have been used in election purposes. So that's an example of what an incident is, and you'll kind of see quite simply. There are more AI incidents than ever before.
- Now, despite all of this, there are kind of pretty substantial gaps that exist among AI benchmarking. So when it comes to general capability, benchmarks like Mmlu or Gpqa. These are benchmarks that look at for example, how well, these systems are able to do college reasoning. How well they can do scientific reasoning. There's pretty strong consensus, right? So virtually every single developer is tested on a certain subset of benchmarks.
- When you flip the script and you look at benchmarks for responsible AI. These are again benchmarks that are looking at how honest these systems are.
- how prone they are to hallucinating, how biased they are!
- You'll notice there really is no consensus. There are some developers that score their models on some of these benchmarks, others still that don't.
- And this is again dangerous, because we want to move to a world in which there are apples to apples comparisons about
- what these systems can and cannot do from responsibility perspectives. And our data suggests that this kind of standardization is still kind of unfortunately lacking.
- Now, academics recently have stepped up. Academics have kind of launched certain benchmarks like helm safety and airbench. These benchmarks are basically trying to
- find kind of single one shop kind of aggregation scores of what the safest model is. But part of the problem here is that you know these benchmarks are coming out with a frequency that isn't matching the frequency with which industry is developing these technologies. So
- the stakes here are a little bit different, and someone has to kind of step up to fill in this gap.
- There's also this question that kind of one wonders here about how do businesses feel about responsible? AI. Well, our data suggests that business is still very early on in their Rei journeys. So if you ask, for example, businesses, what are kind of different AI risks that you consider to be relevant versus risks that you're actively taking steps to mitigate.
- You'll notice that these organizations are much more likely to kind of list risks as being relevant than actually taking steps to fundamentally mitigate those risks.
- And this is, I think, reflective of the fact that a lot of these organizations are still very early on in their responsibility journeys.
- The business concerns that organizations have regarding Rei are also changing. So, for example, if you ask organizations, what is the relevance of a selection of responsible AI risks for their organizations. You'll see that virtually every single risk category has become a lot more salient. But the one category that's seen the greatest year over year increase has been financial risks.
- So this is looking at, you know, are you gonna get an Roi with AI.
- Is there kind of other financial associated loss? So you'll notice here, quite functionally that
- businesses are really kind of thinking about this kind of core question of is AI gonna fundamentally drive business value for them.
- We also see that on responsible AI policymakers are also rising to the occasion. There have been several notable Rei policymaking milestones that have passed in the last year from different organizations like the Council of Europe, the African Union, the G. 7 institutions across the world are really
- understanding that AI is is here, and they're trying to institute the right kind of policies regarding this technology.
- Now, a few more highlights before. I kind of give you guys some time to ask questions, because I want this to be as kind of educational as possible.
- We kind of see this dual trend here, where, although AI is becoming more accessible, industry is still continuing to lead in the development of this technology.
- so another very important chart from the index. This is looking at
- how much it costs to use these models across a variety of benchmarks.
- So you'll notice, for example, that
- certain models, if you kind of fix the level of intelligence like Gpt. 3.5,
- it basically costs around $20 per 1 million token. To use a model like this in November of 2022, and that's now dropped to 7 cents per 1 million tokens. And you see, similarly precipitous drops for other levels of intelligence. So AI is becoming a lot cheaper to use.
- And part of the reason that it's becoming a lot cheaper to use is models are becoming a lot smaller right? So this chart is looking at the size of the largest model that, or sorry the size of the small small that gets above a 60% on Mmlu
- And we see better performing models because
- AI systems are becoming more energy efficient and also just getting better price performance from their gpus. So some of the more recent gpus, like the H. 100 and the A. 100, they deliver substantially more performance than some of the earlier ones that have been released.
- We also see better performing open weight models. So if you kind of juxtapose the performance of the best closed versus open systems. Again, there was a pretty significant gap between. How well the best closed system did versus best open system in January of 2024, that gap has really kind of narrowed down substantially, and there really is not a major difference between the performance of these different types of models.
- Now, despite all of this, industry, is still continuing to race ahead when it comes to AI, there's more notable AI models that come from industry than virtually any other source, and quite notably in 2024 there were 0 notable AI models produced from academic institutions, and when you zoom out in the last decade. It's again, these organizations like Google, Meta and Microsoft that are really leading the charge here.
- And the simple reason why industry is really in a position to dominate AI is because it just costs a lot of money to train these systems. So you know, in 2017, it cost around $1,000 to train these systems in 2020 to 21. The costs went up to 4 to 6 million, and in the last year we've seen costs that range anywhere from 40 to 200 million Us. Dollars. So very substantially high costs
- and
- the high cost really associated with rising compute costs right, these model developers. They know that if they pump more data into the system they could get substantially better performance. And as a result of doing that you've seen as well. That compute needs have also gone through the roof.
- Now I mentioned that AI, as a whole is becoming more accessible. It's becoming cheaper, you know, it's worth mentioning, though frontier intelligence is still relatively high. Right? So if you want to access the best in class models.
- those models tend to be a lot more expensive. For example, April of 2024, the o. 1 model costs around $60 per 1 million tokens to use.
- But again, it is still cheaper to use a really high quality model now than it was before, and the high quality models now are a lot better. So remember that when Chat Gpt came out
- and Gpt. 3.5, I said, cost around $20 per 1 million tokens to use
- deep seek. r. 1 is a lot better than Chat Gpt.
- I'd probably say, 7
- notable kind of developers that are all providing really high caliber AI solutions. And I think it's interesting to kind of wonder which particular developer is gonna win out in the AI production race.
- Now, I mentioned earlier that industry is leading the charge. I think that's true.
- Academics are still pretty well represented when you kind of look at the number of highly cited publications. So it's, you know, it would be wrong to kind of say that academics are completely
- misrepresented, but industry is definitely kind of leading the charge.
- We also have a chapter in the AI Index that looks at trends in AI education. So here we find that although education is expanding, there are still gaps that remain so. For example, at the index, we looked at the availability of computer science education by country. And you'll notice there's different levels of availability. There are some countries that really mandate this other countries that don't mandate it as much. And the levels of availability kind of differ fairly sharply.
- This is despite the fact that there is more post-secondary graduates in computer science than ever before. And alarmingly, a lot of teachers at the K to 12 level simply don't really feel equipped to actually teach AI. This is obviously a huge problem, because we need equipped
- leaders like yourself and young students like yourself, with the ability to think about these tools and to kind of use them. Well, now, last, but not least, we find that when it comes to AI there's still a lot of reasoning challenges that remain so. AI can do a lot of things, but
- it does struggle on tasks like plan bench, which are about kind of measuring how well these systems can plan and kind of reason across different iterations. So there's a lot of things that AI can do. But it's important kind of. In conclusion, to mention, there's a lot of things that AI also cannot do.
- That concludes my presentation. I mean, I've intentionally built in a lot of time for questions, hoping that there was a bit of an active discussion. I will also kind of throw into the Zoom chat the link to the AI index. If you actually want to read
- the report in its full glory. But it's very exciting to talk to you guys today. I hope you've had a good AI for all session, and happy to kind of field some questions about the report and the work that I do. So thank you very much.
- 00:39:04
	Thank you so much, Nestor.
- Students feel free to get off mute at any point, to ask your question, or if you feel more comfortable. You can write it in the chat.
- 00:39:18
	Hi, can you explain the difference between AI reasoning from like o 1, and the kind of generation that comes from like Gpt. 4.
- 00:39:28
	So, o 1! As I said, it's like one of these.
- test time, compute models where, as it is answering a question, it is going to kind of reason, and basically like, break down the reasoning and kind of decompose the reasoning and based on that decomposition further kind of refine its answer. So it takes a lot more time to do, because it's reasoning a lot more intensely. But it tends to basically answer questions a lot better and more accurately than some of these models like Gpt. For that don't have that capability.
- 00:40:09
	We have one question in the chat. I'll read it out loud if the cost for training AI models can be decreased to a reasonable price, so that it is no longer limited to large industries. What would be the larger impact on society.
- 00:40:25
	I think it would just make AI research a bit more democratic because I mean.
- yes, these industry sources are providing the models. But at the end of the day. Industry has incentives to kind of build particular types of systems for particular reasons. They need to sell these models, and there might be a lot of good reasons to build AI systems that only kind of academics are well positioned to do so. I feel that
- this kind of change could really
- lead to a difference in the types of models that are being built and the types of AI research that is kind of pioneered.
- 00:41:17
	Another one in the chat. I'll read out loud as the net 0 carbon emissions deadline approaches. How have legislators tried to address the environmental impact of AI development while still exploring its potential.
- 00:41:31
	Yeah, another really good question.
- I mean, this is challenging, because I think there's 2 sides here where, like on the one side.
- AI takes a lot of energy to train, and the energy to train is kind of associated with the fact that
- you know this transformer model. It is learning a lot of data representations. And it's being fed more and more data over time. The more data it needs to kind of learn on the more computational needs it has and the more energy it uses in data centers. Now, with that said data, centers have become a lot more efficient. So the amount of energy that it would have taken to train some of these systems in the past isn't the case anymore. But the systems are growing in size.
- So the energy demand is definitely kind of going through the roof.
- Now, on the other hand, AI is now being used a lot more to find ways to kind of save and conserve energy. There is this interesting study, for example, that came from deep Mind, where they basically tried to use AI to modulate the cooling levels of buildings to basically say, like, you know, people want buildings cooled to this temperature and find a way to keep that temperature while kind of optimizing energy usage.
- And they've basically figured out that they can cool the buildings.
- and in some cases, save, you know, 15 to 20% of energy. So it really is a mixed bag, you know. I think there's different forces that are at play. I would say that in the short term I think AI is probably going to take a bit more energy, but in the long term, if used correctly, it can really help save a lot of energy as well.
- 00:43:13
	And what are some of the most surprising trends you have seen in the latest AI index.
- 00:43:19
	I think it's really surprising to see.
- I think, just how much businesses are getting on board. I think that anytime a new technology comes, there can sometimes be a bit of a lag between when businesses actually fully embrace it and fully put themselves in a position to use it. It seems like, really, businesses are quite excited about the tech and are wanting to use it quite a bit. And I think, basically just kind of seeing the pace with which they've embraced this technology has been
- very encouraging and very exciting, to kind of say the least.
- 00:43:59
- 00:44:08
- Now, a lot of these cases are still in court, and I think fundamentally, it's going to take time for lawyers to kind of figure out what the law can actually say. And in some cases it's going to be up to judges to basically kind of make that decision about
- what can and cannot be done with these systems. But I think the kind of short answer is, there's a lot of questions and concerns, but no one really kind of quite knows exactly what the law is saying and how that might kind of shake out. So it's something that's really kind of worth keeping an eye on in the near future.
- 00:45:12
	Thank you. Do you know, if a lot of schools with global or will global apply restrictions of AI at a certain point.
- 00:45:23
	Yeah, I mean, I think there's gonna be.
- I think there's gonna be different rules about how the technology is gonna be used.
- Now, I don't know what exactly is going to happen, but I think some kind of regulation should be put in place, I think, on several factors. I think one is the fact that.
- you know, students are inevitably gonna use these tools. And you know, AI could write essays. Well, it could code. Well, it could do a lot of these things.
- and on the one hand, you can't kind of keep your head in the sand and pretend that these tools aren't here.
- But if you want students to learn at a high level, you perhaps have to reimagine new ways of learning to ensure that students can kind of adapt to some of the challenges that these technologies have have created and posited.
- So I think that's 1 thing. On the other hand, you know, the technology is here. It's not going away. And there's this kind of quote from Eric Brunelsen, one of the steering committee members of the AI Index that I really like, where he says that you know AI is not going to take jobs
- away, but it is going to take away jobs from people that don't know how to use the tools. So you really need to kind of find ways to use the technology and to kind of understand what it could do. So that there's that kind of balance where I think educators across the globe. They have to think of the right rules, but make sure that those rules are kind of in keeping with getting students to be on board with the tech and wanting to use the tech well.
- 00:46:51
	Perfect. We have a few folks with hands raised so I think, Jasmine, go ahead and go first.st
- 00:46:57
	Hi, I'm Jasmine and I had a question about AI regulation. That is, do you think like the current regulation on on AI is like keeping up with its development. And what would happen if these like regulations aren't implemented quick enough.
- 00:47:14
	I mean, I think it's probably not keeping up as quickly as we would want, because AI does develop very quickly. But I do think that relative to other technologies, it's kept up a little bit more so. I think a parallel that I like to draw is that
- you know, if you consider 2022 to be the kind of landmark moment for AI.
- We saw major regulation from Europe with the Euai act as well as Biden's executive order on AI in 2023.
- So Regulators have stepped up, but
- if people aren't trusting the technology, there might be negative effects that come from it, and it might be hard to then get people to embrace it. And I think in kind of landscapes where the regulation is poor. You might have misuses of the technology that are a little bit more prominent. And as a result, there's going to be less and less trust regarding it. And that's something that's kind of quite damaging, and you know, unfortunate.
- 00:48:49
	Thank you.
- 00:48:52
	Yeah, I'll hop back to the chat, and I'll summarize one of them. What do you see as the real risks if we keep building powerful AI that we can't fully explain.
- 00:49:05
- if AI is put in a position to make a lot of very important decisions like, decide whether you get approved for a loan or decide whether you get into a school or decide whether you get into a job. If we don't really fully understand the decisions it is making. It can be easy to kind of get disenchanted with these systems to feel that these systems are less and less fair, and to kind of
- become a bit bitter about the technology and the role that it's playing. I mean at Stanford High. I think we fundamentally believe that AI as a technology has the potential to revolutionize the way in which the world is going to work. But if we're gonna have this technology serve this revolutionary function. We basically need to have everybody get on board with it and what it can do.
- And if there's not enough good regulation, then people might not be as excited, and there might be kind of some negative downstream consequences that flow from that.
- 00:50:06
	Thank you. Do you think that the development and improvement of AI will plateau anytime in the near future, leaving us with our only remaining area of improvement to be an issue of collection of data.
- 00:50:20
	Great question. I mean, I think that the technology will probably still continue to get better just because there's a lot of people in industry that are paid a lot of money to continue improving it. I think there's a fair question to say how much better it's gonna get. But I think, even if the underlying technology doesn't improve, I think
- there's going to be improvements in business processes and kind of usability that is going to drive a lot of productivity gain. So I mean, historically, it hasn't always been the introduction of new technologies point blank that has led to massive gains in productivity. It has been more so, the introduction of new technologies, and then the discovery of ways to use and apply that technology. A good example is electricity, right where, when electricity was discovered in the late 19th century.
- it probably took
- a good 4, 5 decades before it started having pretty substantial productivity impacts, because what factory owners did is they had steam powered factories, and they would just replace the steam powered factories with electrical ones, and that was marginally better. But it didn't make a huge difference.
- Now, it took a few decades for people to figure out that if you actually rebuild factories so that it's based less on a central mode of power operation more decentralized, which you could do with electricity. Then you started to see kind of doubling and tripling of productivity. So I think similarly with AI. You know, the tech, I think, will continue getting better. But even if it's not going to massively improve.
- if we just kind of
- at some fundamental level find better ways to kind of use it and deploy it. Then I think we could still see a lot of really positive benefits.
- 00:52:09
	Sammy! Go ahead.
- 00:52:13
	Okay. So I had a question about like the actual like AI index report. You know, it's like, really big. And there's like so much data in it. Like, what were the challenges you guys faced, you know, with like, actually like collecting the data and like making sure that it was like credible, I guess.
- 00:52:30
	Yeah, I mean, I think there's always challenges about vetting it. I think there's challenges now about what do we actually decide to include? Because there's just so much things that are happening with AI that it's hard to know what to include and what not to include. Now. So I think that was a really big challenge that we encountered and that we faced.
- And I think also.
- there's still some questions that we would like data on where it's just really challenging to get this information on. For example, you know, we would love to know at the AI Index
- how much it costs to use some of these systems in different use cases. We don't have that data. We would love to know, for example, how much businesses are using these tools across different geographies across different functions.
- Okay.
- 00:53:39
	Follow up.
- Do you know how much data like? Do you know how many like businesses actively use the AI index report.
- 00:53:49
	It's a good question. I wouldn't know. I mean, I like to delude myself and think that it's millions of thousands, but I mean, we've presented to a lot of businesses, you know. I was with Miriam
- in the Middle East
- a month or so ago, presenting to some kind of clients for accenture. And they're kind of development ecosystem there. So there's definitely a lot of different people that we kind of try to present to and speak to about this technology and what it can do.
- 00:54:18
	Yeah, one more question in the chat. So specifically, in areas of equity, safety and governance. What trends in the AI index surprised you most.
- 00:54:28
	I think it's probably just that safety, I think, has been a lot less prioritized in the last year, and I think it's interesting, because I think when AI was 1st launched in 2023, like not 1st launched, AI has been around for a while when Chatgpt came out. I think one of the 1st reactions was, you know, everybody was freaking out. These systems might kill us right? There was a lot of notable AI luminaries that signed this letter being like, we need to pause AI development for the next 6 months.
- You had the 1st AI safety summit in Bletchley, and it seems like there's been a you know. I don't. Wanna.
- I'm a i guess I'm a millennial, and I'm not even sure what generation you kids are. But you know, trying to keep up with the language that you use. There's been a bit of a vibe shift where it seems like safety has been a lot less prioritized.
- You know Paris. There was a summit there with AI in February. It was the AI safety summit. 1st it got rechanged to the AI Action Summit. The AI Safety Institute in the Us. Has been rebranded as the AI Innovation Institute. So
- it seems like there's a bit of kind of a a shift. That's kind of going on kind of in attitude, and that has a lot of important implications.
- 00:55:40
	Thank you so much, Nestor students, if you have another question, I think we have like a few more minutes to get it in or feel free to again unmute and just ask.
- 00:55:52
	I'll also leave my email in the chat, just in case any of you guys want to kind of follow up at some point with any other questions.
- 00:56:00
	Thank you so much.
- Perfect.
- Okay, we did get one question. Do you think the semiconductor boom has partly influenced policies like Trump's tariffs as a way to encourage more domestic semiconductor production.
- 00:56:16
	Yeah, I mean, certainly, we obviously need these semiconductors and Gpus to train AI systems. And I think there's a demand to try to increase the training capabilities and the development capabilities in the Us. But it's hard to just kind of get that going off the ground in a simple way. So I mean, they're starting to do that. But it's gonna take them a while to kind of catch up to Taiwan, and the capabilities that Taiwan has.
- 00:56:41
	Thank you. And do you think AI will take over research? Will it become independent, like at its own data, to improve itself.
- 00:56:50
	I think it's gonna help research. I still think we're kind of far away from these like operating AI systems. I think AI is very good at these kind of narrow tasks, like coding copywriting writing, but, like humans, are very good at like flexibly intelligent tasks, and I still think their AI systems are kind of quite far behind.
- 00:57:19
	2 more questions come in. Okay. AI models perform well on narrow tasks, but still struggle with complex reasoning. Do you think scaling exists? Architecture. Existing architectures will be enough to solve this? Or will new approaches be needed.
- 00:57:36
	I think new approaches will be needed. I think scaling will lead to better systems. But I think that if you look at the human brain. It takes a lot less energy to train us. And we're substantially more flexible. So there's clearly kind of some architecture that's powering us that isn't represented in the systems, and I think when it's discovered, it will drive a lot of value. So I do think there has to be new ways of thinking and developing these systems.
- 00:58:03
- inequality?
- 00:58:13
	Good question. I mean, if you think about AI as something that has a lot of potential to impact productivity and level countries up countries that are leading with AI and kind of imposing it more and working with it more will be much better positioned to kind of use the technology and develop and leverage its advantages. So I think the kind of uneven development can have a lot of impacts. And it's something to keep an eye on in the future.
- 00:58:43
	Thank you. We have time for just one more question. If anyone wants to hop off.
- off, hop off mute.
- Okay, I'll I'll just read the last one from the chat. At what point, if there is one, is it considered unethical to continue investing in the improvement of AI despite its various impacts. IE, environmentally.
- 00:59:05
	I mean, I think this is a hard question for me to answer, because ethics is fundamentally up to the beholder. Right? I think some people there are some people in the AI Safety community that you know really feel. AI is going to kill us all at some point and think that any sort of AI development is unethical or unwise. Other people think that that's just kind of needless fear mongering. So the question of ethics is a difficult one, but it's ultimately one for people like you to decide right? I think this kind of speaks to the mandate of the Index.
- You know. We believe that AI is kind of a human question. We want to give people the data to answer those questions as well.
- 00:59:42
	Thank you so much, Nestor, for your time, and answering all of these really amazing questions from the students. I really appreciate it, and thank you for sharing your email. Everybody give a electronic round of applause for Nestor.
- 00:59:55
	Thank you guys for having me. It was great to speak with you.
- 01:00:00
	Thank you. Everybody. Have a good day. Goodbye.
- 01:00:04
	And just quickly, before we all sign off here we are going to head into our reflection sessions. We are reaching the halfway point of the program already. And at this point we ask students to just give their feedback in the form of a mid program survey. So I'm going to put that link inside the chat.
- and the end of the day tomorrow, and then also, I'll put in the chat the links to the reflection sessions
- which are next. And for today's reflection sessions. You started in your workshop session this morning, or, you know, depending on where you are in the world. Earlier today, you started talking about your projects and the problems that your projects are addressing. So in reflection sessions today, you're going to continue that conversation in your new groups
- and present, if you will, some of your thoughts around the discussion that you had from your earlier conversations with your teams. So I'll go ahead and drop the reflection session links into the chat.
- Your sessions begin in about 9 min. So take your time again. You can complete the survey if you'd like, and then your mentors will see you in your reflection sessions at 2, 10,
- or 10 after the hour.
- Thank you so much, and let us know if you have any questions.