---
title: "Video Conferencing, Web Conferencing, Webinars, Screen Sharing"
source: "https://stanford.zoom.us/rec/play/n8wMHnSFHQdeGrGOtblVtbHffjrEw1p1TpoPw0zqW-7TLUMx4tb5baEG1hm_BdjNCiMF_RtOE88okZXn.VkTmneInfI0siEfF?eagerLoadZvaPages=&accessLevel=meeting&canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fstanford.zoom.us%2Frec%2Fshare%2FL_IX0xv5TEpPsBRe9jgaxfuZw4ic9hnHepve1arrNc56OPmQTAN8zE4irqz-PqKd.TGoKfC-xIt4Cmsno"
author:
  - "[[Zoom]]"
published:
created: 2025-07-04
description: "Zoom is the leader in modern enterprise video communications, with an easy, reliable cloud platform for video and audio conferencing, chat, and webinars across mobile, desktop, and room systems. Zoom Rooms is the original software-based conference room solution used around the world in board, conference, huddle, and training rooms, as well as executive offices and classrooms. Founded in 2011, Zoom helps businesses and organizations bring their teams together in a frictionless environment to get more done. Zoom is a publicly traded company headquartered in San Jose, CA."
tags:
  - "clippings"
---
## Summary
Dr. Olga Russakovsky, a professor at Princeton and co-founder of AI for All, delivered a welcome session for Stanford AI4ALL 2025, focusing on fairness and visual recognition in AI. She argued that AI systems are not just 'models + data' but fundamentally 'models + data + humans,' emphasizing the often-overlooked human element. Dr. Russakovsky highlighted pervasive social biases in computer vision datasets (e.g., facial recognition, gender stereotypes, geographic disparities) and their propagation into AI models. She discussed mitigation strategies, including data examination, representative data collection, and algorithmic interventions, while acknowledging their limitations. The talk underscored the critical need for diverse backgrounds and perspectives among AI developers and leaders to address bias and ensure responsible AI development.

## Key Points
-   **AI System Components:** AI is fundamentally `models + data + humans`, with the human element being crucial but often overlooked.
-   **Social Bias in Computer Vision:** AI models exhibit biases (e.g., facial recognition accuracy disparities, gender stereotypes in activities like cooking, geographic representation imbalances) due to skewed training data.
-   **Data Set Examples:**
    -   **Gender Shades Study:** Commercial facial recognition systems perform poorly on women with darker skin (13-14% error) compared to men with lighter skin (<1% error), linked to data sets having 80-90% lighter-skinned faces.
    -   **ImageNet:** Skewed gender distribution (more male-coded images), age bias (more 18-40 adults), and cultural bias (e.g., \"groom\" depicted as Western heteronormative).
    -   **COCO:** Gendered associations with scenes (e.g., men in outdoor sports, women in shopping/dining) and stylized depictions of co-occurrences (e.g., women with flowers vs. men with flowers).
    -   **Geographic Bias:** Standard datasets heavily concentrated in the US, leading to models failing to recognize common objects (like bar soap) from other regions.
-   **Mitigation Strategies:**
    -   **Know Your Data:** Thoroughly examine dataset contents.
    -   **Collect Representative Data:** Crucial but difficult and expensive.
    -   **Algorithmic Interventions:** Methods like reweighting or using generated data can help but have limitations and can introduce new biases.
-   **Bias Propagation:** Bias can propagate when AI models are powerful enough to capture data patterns fully and when data bias is salient to the model's task.
-   **Importance of Human Perspective:** Bias originates from human history and prejudice, propagating through data, models, and AI decision-making. Diverse AI researchers and leaders are essential to identify and address these biases, as lived experiences inform the questions asked during development (e.g., Joy Buolamwini's facial recognition work, speaker's experience with voice recognition).
-   **AI for All:** A non-profit dedicated to increasing diversity and inclusion in AI, aiming to educate a diverse next generation of AI leaders.

---

https://stanford.zoom.us/rec/play/n8wMHnSFHQdeGrGOtblVtbHffjrEw1p1TpoPw0zqW-7TLUMx4tb5baEG1hm_BdjNCiMF_RtOE88okZXn.VkTmneInfI0siEfF?eagerLoadZvaPages=&accessLevel=meeting&canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fstanford.zoom.us%2Frec%2Fshare%2FL_IX0xv5TEpPsBRe9jgaxfuZw4ic9hnHepve1arrNc56OPmQTAN8zE4irqz-PqKd.TGoKfC-xIt4Cmsno
# Original Content

[Accessibility Overview](https://stanford.zoom.us/en/accessibility)

<video src="https://ssrweb.zoom.us/replay02/2025/06/25/381BB9FD-33CC-4872-8A51-620B8112953B/GMT20250625-194837_Recording_1920x1108.mp4?response-content-type=video%2Fmp4&amp;response-cache-control=max-age%3D0%2Cs-maxage%3D86400&amp;data=ba89b7a4df6f0b14f6c49cce8fafe06ecf7a69d8d0c89f91def92d21f26953e0&amp;s001=yes&amp;cid=aw1&amp;fid=B6y7stD0ugAU0ucBi3V-NA9hvFKuNfwaxhs6aWIBZoRIBSLS4ISe6rViUXwchzwcV5JgGKMc897Z4Vbn.vMtZxwZHouSO0HDg&amp;s002=D5eoRlERELfE6-Vv11WqGXAv6yIYv257qsNZ6dNaAW5SLSFvIkWUM-5WXo7rNSjARvYHoIG5K7wl9T8gEQDyLXLqTVDG.tAUC09VEeKKgj-eu&amp;tid=v=2.0;clid=aw1;rid=WEB_97a9f2718fd097702bd0b17af56e0bd7&amp;Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6Imh0dHBzOi8vc3Nyd2ViLnpvb20udXMvcmVwbGF5MDIvMjAyNS8wNi8yNS8zODFCQjlGRC0zM0NDLTQ4NzItOEE1MS02MjBCODExMjk1M0IvR01UMjAyNTA2MjUtMTk0ODM3X1JlY29yZGluZ18xOTIweDExMDgubXA0P3Jlc3BvbnNlLWNvbnRlbnQtdHlwZT12aWRlbyUyRm1wNCZyZXNwb25zZS1jYWNoZS1jb250cm9sPW1heC1hZ2UlM0QwJTJDcy1tYXhhZ2UlM0Q4NjQwMCZkYXRhPWJhODliN2E0ZGY2ZjBiMTRmNmM0OWNjZThmYWZlMDZlY2Y3YTY5ZDhkMGM4OWY5MWRlZjkyZDIxZjI2OTUzZTAmczAwMT15ZXMmY2lkPWF3MSZmaWQ9QjZ5N3N0RDB1Z0FVMHVjQmkzVi1OQTlodkZLdU5md2F4aHM2YVdJQlpvUklCU0xTNElTZTZyVmlVWHdjaHp3Y1Y1SmdHS01jODk3WjRWYm4udk10Wnh3WkhvdVNPMEhEZyZzMDAyPUQ1ZW9SbEVSRUxmRTYtVnYxMVdxR1hBdjZ5SVl2MjU3cXNOWjZkTmFBVzVTTFNGdklrV1VNLTVXWG83ck5TakFSdllIb0lHNUs3d2w5VDhnRVFEeUxYTHFUVkRHLnRBVUMwOVZFZUtLZ2otZXUmdGlkPXY9Mi4wO2NsaWQ9YXcxO3JpZD1XRUJfOTdhOWYyNzE4ZmQwOTc3MDJiZDBiMTdhZjU2ZTBiZDciLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NTE2NDQwMjV9fX1dfQ__&amp;Signature=bR-ouzaZV27bYTNk2P03MHfV9RmLxdlr~uTdaKI1RPOzlLct2YToFb1W425YJwF46TuOeFIN-vZY6ZDVo7d4-X4PPOcr6j4r5Js0DKnLU-Qa3~w~chRH~6jL0EZIIdzsFouZXcxMjIpLwObGoituYDeS11LNFFbiCmdqBB9x8gZJ2yjHuPQnIHw5PBs0ksQhvzSCD~xv6lk1vcgfmcTodmyx6VUlFpp-0zGGWwGcBmdXcd8uWIFLzAGu4myLnhLBwWB5aswW9ZbigPuZu7Zg6lqTpgitYxEUYYOJjQKvn82vcJijkMs92kqBagbelFQQs2AhHwwZyWsXyMgHdUN-gw__&amp;Key-Pair-Id=APKAJFHNSLHYCGFYQGIA"></video>

Oh my god! Fire everybody!

00:00:00 / 01:00:15

## Audio Transcript

## Chat Messages

- 00:00:06
	All right. Hello, everybody welcome. I'm going to introduce our professor for today. Dr. Olga Rosakovsky is an associate professor in the Computer Science department at Princeton University, as well as the Associate director of the Princeton Laboratory for Artificial Intelligence. Her research is in computer vision, closely integrated with the fields of machine learning, human computer interaction and fairness, accountability and transparency.
- She has been recognized with the presidential early career award for scientists and engineering. In 2025. The Pami Young Researcher. Award in 2022, the Anitabees, dot Org's emerging leader, Abby award in honor of Dennis Denton. In 2020,
- the Crawp Anita Bourke. Early career award in 2020, the Mit Technology reviews, 35 under 35, innovator award in 2020 17, she has served as one of the program chairs for Eccv in 2024. In addition to her research.
- she co-founded and currently serves as board chair of the AI for all nonprofit dedicated to increasing diversity and inclusion in AI. So everybody welcome associate Professor Olga Rosakovsky.
- 00:01:20
	Oh, goodness.
- Hello, everybody! It is! I love the idea of like everybody posting high ends in in chat that was awesome, that I've I've given so many zoom talks. I've never had that happen. And this is just beautiful. And now I'm going to make all of my students do it. Now, anytime. We're hosting, you know, a virtual speaker. I'm gonna make everybody do this. This is just really really lovely.
- chat. I'm really, really excited to be here. I brought stuff for show and tell. So, showing Miriam in Brooklyn ahead of time. So this is the 1st year of Stanford AI. For all signed by all the students. So this is a photograph that still sort of lives in my office. This is it was the 1st year that we did this, and sort of had 24 High school girls come to Stanford and
- hang out with us in the AI lab and learn about AI, and then this is so back in the day the program was called sailors. So this is why this little is like a little stress ball. Robot. I don't think they make them anymore. We used to print them, for, like every year of
- of the programs. We would like print a new one. But I don't think these are being made anymore, which is really unfortunate, because it's very cute, and then it says, Sailors, which was the Stanford AI. Lab Outreach Summer program, which is what it was originally called. So I still have these in my office, and I am just absolutely delighted. The program is still going. And I'm just, you know, here to make this experience
- good and happy and educational and enriching for you all. So I will start sharing slides and dive in. But please feel free to
- pause me like please feel free to go off on tangents. If there's something that's like on your mind that you've always wanted to ask somebody like, Please feel free to ask, and if we don't get through the whole talk, that's totally fine, and if we you know, get through it all, and then have more time for questions. That's also fine. So so, however, this goes is great. I probably won't be able to see chat, but I will be able to see, I think, hand rate.
- hands raised because I will. I do see some of the videos. And so I think if you raise your hand it'll pop you up to the top. And so I think I will see that or feel free to just unmute and jump in.
- Also totally fine.
- Okay, so
- I'm going to dive in. So what I wanted to talk about today again, unless we get distracted, is talking about fairness and visual recognition. In particular, the importance of the human perspective and computer vision. So I am a computer vision researcher, I run a research lab, it's called the Visual AI lab,
- you can, you know, find us online learn a bit more about what we do. But of course, the sort of most important part of any lab, or the people who work in it. So this is a photo taken now a couple of years ago. So most people are still around, but not everybody. And then we've got new members. But this is us sort of sitting around a table at a dinner. And what we're trying to build is computer vision systems. So sort of AI systems that can
- understand different parts of this image, for example. So be able to say that there's a fireplace, and like there's a box of Legos, because, of course, I'm holding a box of Legos. Why wouldn't I be? There is some glasses in the foreground, right? So solving object recognition problems building AI systems that can do. Scene understanding so sort of figure out that this is a dining scene that we're all sitting around the table, that there's a person wearing the glasses.
- being able to do action, detection. So on images or on videos. These are people who are sitting and smiling and maybe posing for a photograph. So, what are the people doing, being able to do vision and language? So, for example, being able to describe this photo in natural language, or even perhaps being able to do image generation. So sort of go the other way is given a natural language description, being able to generate an image.
- so I will. By the way, I should probably ask, how far are you into the program? Like, what what can I assume? Are you like
- this? The 1st week? Is this the last week, like, where, where are you at.
- 00:05:36
	About halfway through.
- 00:05:38
	About halfway through. Okay, okay, a good sense of that. All right. So
- And so when it comes to trustworthy, we might think of things like capable or robust, that when it comes to trusted, we might think of things like transparent or responsible, and then sort of at the intersection is, we would think about things like thoroughly evaluated or thoroughly understood. And so a lot of the type of work that we do is
- like really trying to get to the heart of like, what makes these systems thing like? Sort of why do they work the way that they work? And what about what about the sort of components of the computer vision system are actually important.
- And so if we think about sort of building an AI system. And here I'm sort of assuming AI is equal or approximately equal to machine learning. I know you all have talked about sort of what does AI mean, and what is sort of the basics of machine learning. How does AI relate to machine learning? We can have these debates. And I've had many of these debates about sort of is AI bigger than machine learning is machine learning bigger than AI are. They sort of is one a subset of the other. I think everybody has different opinions. So I'm happy to
- have that debate with you also. But if we just sort of simplify them, and then think of it in the context of AI being sort of approximately equivalent to machine learning. You're trying to build an AI system right? For example, you wanted to answer questions like, what objects are in the photograph, or you wanted to answer sort of maybe a broader range of computer vision questions. But at a high level, what you're going to go out and do is collect a set of photographs that
- represent sort of your domain of interest that represents the kind of things you might want to recognize.
- You're going to take some model architecture. You're going to adjust the models, parameters. So you're going to sort of train this model such that given and sort of compress all of the information from this data into the model parameters, such that, given a new photograph, the model could automatically say, Okay, this is car person building, I will say, just sort of as a
- broader reflection. I mean, I've been using this slide since 2014,
- the more that are like, you know, it's it's evolved a little bit. But like
- this basic idea I've been using since 2014 when I 1st started giving like versions of general computer vision talks. And so I think there's a lot of
- conversations and a lot of like in the media and in general about how AI now is completely different than it was 10 years ago, and like it really isn't
- like this paradigm held in 2014, it held in 2,004. It is holding now in 2025 like it's, you know. Yes, the model architectures have changed like, yes, the data has gotten bigger. Yes, the compute has gotten better. But like this core idea has really not changed. And so a lot of the things that we're talking about here honestly, they were
- true again 10 years ago. They're going to be true, going forward. So
- in many ways, sort of despite a lot of things in AI are changing, a lot of things are also kind of staying the same.
- So this picture, right at a high level, consists of 3 big pieces, right? It consists of the data piece. So there's some kind of collection annotation, evaluation using data. There's the model piece, which is sort of architectures or optimization algorithms sort of how do you build systems that learn from this data. And then there's sort of the human piece. So the human judgments, or human opinions, or sort of somebody who comes in and decides what is what is going to be built.
- And so there's a lot to be said about sort of power structures. And AI, if we just focus on this like human component, which I think is the piece that's most often overlooked. Right? When we think about AI, we'll think about data and models and maybe compute. But, like actually, this, human conversation is a lot
- less prominent. But I would argue at least as important, right? So some of these questions about like, who decides what problems to tackle next with AI interventions like who proposes and implements AI solutions, who determines and oversees AI ethics.
- who governs these systems? And also particularly sort of international like, what does this mean to govern AI, which is
- international technology and like an international research community? But like.
- how do we make sure this is deployed safely also, very importantly, who has the resources to contribute to AI research like AI research is very, very resource intensive, who gets overlooked, or perhaps even exploited with AI progress. So I'm not actually going to
- try to tackle these questions. I'm sort of posing them as something to think about. And I know this is something that you are talking about in the context of Stanford AI. For all but what I want to focus sort of for now, and I'll come back to this a little bit at the end of the talk. But I want to focus on sort of the extent to which kind of humans. But beyond AI researchers are part of technical AI research. So when we're doing the engineering work that we're doing of building AI systems or computer vision systems.
- To what extent sort of do we touch on some of these more human questions? And to what extent do we touch on questions about representation or questions, sort of about the effect on downstream users and things like that.
- So oh, so AI models now are everywhere. And you know we've been saying this for years.
- Now, this is actually true. This is one of the things that I think has changed. So sort of the deployment of these systems, both in things like facial recognition, you know. Unlocking your phone like, I unlock my phone right with my face. Now, every you know there, there's all kinds of AI technology all around us. And then, of course, large language models, of course, chat, gpt, etc, etc. that that people are using all over the place.
- So
- the context of this sort of data model in humans, one of the places where you see this most clearly is when we start asking questions about representation within computer vision systems. And when we start asking about sort of fairness and bias within these systems. So around 2017,
- and so the models were not really being deployed at scale and sort of around, you know, basically around, you know, 10 years ago, give or take
- putting all of that garbage aside. But also it turns out what it was doing is making your is manipulating your skin color to make you more attractive, which is just
- okay. We're being recorded. So I can't use, you know, the kind of language I would want to use here, and I guess you know high school students, I will refrain. But but this is not okay. In many ways.
- the one that looks lighter, slimmer, and younger. And there's a number of books and documentaries and sort of investigations around this. These are a few that I would very highly recommend. If you're interested in these topics, one is algorithms of oppression by Sophia Noble looking at web search engines, and how they sort of
- betray different
- sort of stereotypes around different groups of people, particularly focusing on black girls. Race after technology, which is a book written by my colleague, Professor Ruha Benjamin, again, sort of making a number of interesting observations about how technology perpetuates
- societal inequality in ways that sort of perpetuates and amplifies it in ways that are very consistent with historical patterns. And so, even if we think that this technology is new. And it's going to do something new and different. Actually, a lot of the things that it's doing is sort of
- continuing some of our old patterns of racial injustice, and she's actually written a number of books since then, which I also highly recommend all of them. She's just like a wonderful writer, and then coded bias, which is a documentary on Netflix. If you're not so much into reading or listening to audiobooks, I know both of these are audiobooks, because I've listened to both of them, because I'm also not very good at reading, and much better at audiobooks. But if you don't like
- books in any form coded by as a documentary on Netflix, by Joy Bulamwini, or sort of about joy. Bulimwini, and some of her work is is also very very good for
- for deep dive into these topics.
- But I guess if we're gonna sort of dive into the more technical or more sort of engineering part of this right? So where does some of this come from.
- and kind of at the heart of, or one of the sort of root causes of these kinds of behaviors by the models is comes in the data so large scale data. So the fact that we're training these models with tons and tons of data does not necessarily mean that there's like equal representation in the data, or does not necessarily mean that the data is capturing the kinds of patterns in the world that we would like it to capture.
- So, for example, the work that I just mentioned, the facial recognition is accurate. If you're a white guy. Well, so these 2 researchers so Joy, the one who later did the code advice documentary and Denit Gibru did this work in 2018, called gender shades, where they analyzed the performance of face recognition algorithms
- of commercial face identification systems. And what they found is that these systems perform very poorly on women with darker skin, and by very poorly, I mean around 13 to 14% error. And in contrast, when these systems sort of operated on faces of men with lighter skin. The error rate was less than 1% right? So sort of massive disparities and error rates.
- and one of the kind of reasons why this was happening is they kind of traced it back to the fact that within face recognition data sets that were out there at the time about 80 to 90% of them contained faces with lighter skin. So it's sort of
- one of the kind of 1st lines of defense for analyzing why these models perform the way that they are is looking at what is actually hiding in the data that the models are learning from.
- So another sort of example of this is that there's few demographic distributions and cultural associations of some of these data sets. So this is a data set called Imagenet, and both sort of the the graphs I'm showing on the X-axis. This is sunset. So this is sort of the categories within this data set. These are sort of different categories
- corresponding to different words that might describe people. The Y-axis is percentage. The graph is sort of a histogram of both sort of gender distribution among photos in the data set as well as age distribution of the people that appear in the data set. You can see the gender distribution sort of skews male. I should say that this is
- sort of annotated, inferred gender expression, sort of binarized gender expression that that's being annotated. So not necessarily gender identity. Not necessarily biological sex, not necessarily
- sort of any of those. And again, that's a sort of binarized to be male or female. You might notice one of the categories. Sort of, there's a lot of examples marked unsure about the gender. This is the category scuba diver.
- So most people are in scuba gear, you actually can't tell. So this is why most of those are unsure similar with age. Right? A lot of the people in this data set are adults, 18 to 40, less representation of adults over 40, some representation of children and minors, which is the whole other thing of like what does that?
- What are the pros and cons of having children be in computer vision data sets, but also sort of cultural associations. So, for example, for the category groom, this data set represents it primarily by photos of men in Tuxes, standing next to women in white puffy dresses, which is both, of course, a very like heteronormative, but also a very particular cultural
- depiction of groom. That may not be, you know. It's a very Western sort of us European version of of this category.
- So and finally, sort of one more example. So this is again, another common computer vision data set called coco common objects. In context, where you can see sort of in the graph, right? The Y-axis is the different types of scene that the images come from. So there might be mountain desert and sky scenes. There might be scenes kind of being outdoor next to water, ice or snow, industrial and construction scenes.
- and then the X-axis is the fraction of images with the scene that appear with a person who appears to be male versus the person who appears to be female, and you can see that for some of these scenes. Okay, I don't have this animation, but for some of the scenes like outdoor sports fields and parks, these pictures appear prime. These sort of
- pictures that come from the scene appear primarily as people who appear to be male shopping and dining scenes at the bottom appear primarily with people who are female. And the example on the right is even for some.
- even for some
- sort of some some situations where maybe the distribution is actually equal. So, for example, this is people who appear with flowers. So there's about same number of men and women who appear next to flowers. But when you look at like how they're depicted. So women next to flowers
- tend to have this like very
- Sort of stylized and very like paper
- like, I don't even know how to describe these images. I mean, you can see them in the top row. Sort of these tend to be the images of like
- women appearing with flowers, whereas men appearing with flowers tend to be like men in conference rooms with a flower on the table. Right? So, even though sort of the statistical patterns of co-occurrence appear to be similar, actually kind of the images and the depictions can be very different.
- And so then it sort of comes as no surprise that when you train on some of these data sets that the models themselves may be learning particular patterns that may be sort of hard to
- grasp, hard to grapple with, and may have sort of behavior that may not be particularly
- desirable, and so so sort of what can be done with that? Right? So so step one is like, know, your data is, figure out what is actually in your data set, and as much as possible like you can build
- tools and
- sort of visualization frameworks and things like that that allow you to more closely examine your data and sort of understand what what might be in there. You can, of course, collect more representative data sets. So so when you are aware of something like, Well, I'm trying to build a face recognition system. And, like all of the faces in the data set are lighter skin, right? And I'm trying to get this something to work for. Everybody like that is going to be a problem. I need to have a data set that's more representative of everybody that I would want the system to work for.
- You can create algorithmic interventions, which is what we will sort of talk about here a little bit, or you can, of course, also move away from reliance on data that's more of a longer term strategy, and maybe something that
- you know we can come back to and and discuss more, because this this is something that I think the whole AI community sort of talking about in various ways. But it's not clear that we know how to do that.
- It correlates your target label. So basically something that you're trying to predict. So whether the person is doing a particular activity or whether the person is a
- groom or not, or whether the person is
- sort of in a particular setting or something like that, and it correlates a sort of target label with a protected attribute. So, for example, the person's apparent gender.
- or the person's skin tone, or things like that. So
- I should say that this is a very simplified setting for how you can begin to grapple with something that's very complex. Sort of, you know, this concept of AI fairness is a very complex thing, even sort of this process of simplifying this down into an engineering to a mathematical framework can be very fraught, can be potentially sort of introduce a lot of
- complications and a lot of challenges. So, for example, sorry, this is sort of me going off on a tangent. But there is some work that shows that if you're trying to mitigate bias with respect to
- sort of the gender attribute in some some data sets under some some conditions. So one of the things that you might do inadvertently is like, even though you might mitigate the sort of equalized error rates across the genders. What might end up happening is that the way you're doing that is actually sort of disproportionately harming particular minority subgroups of the 2 genders
- sort of in a way that like okay? According to this one metric that you define as your fairness or bias metric, you might be doing better. But actually, if you sort of break it up into like further break it up into minority subgroups. You might actually realize that.
- Yes, your algorithm has optimized for this particular metric that you set out to optimize. But actually under the hood, it's doing something that's still very problematic, according to other metrics. So this is part of why, whenever you're talking about AI fairness, you actually want to be thinking about a broader range of metrics rather than just one. But
- as one way of conceptualizing this of sort of simplify this, and beginning to even grapple with this problem. Right? You could say, Okay, maybe I'm sort of given this data set. I'm considering a protected attribute. Maybe this is just a binary attribute. Maybe I'm going to simplify gender down to a binary, and my goal would be to sort of develop a method for training visual classifiers that would ignore such correlations if the correlations are deemed unnecessary or harmful.
- And so one of the earliest works on this is actually, it's called men also like shopping sort of reducing gender bias in computer vision models. And this is the work that I sort of briefly mentioned in the in the headline of models trained on computer vision data learned a sexist view of women. So this is one of the early works in this that was showing that basically when
- when a model was trained on a common data set to predict sort of the what action the person is performing, it learned to associate the action of cooking with the person in the image being a woman, and not only that, it sort of amplified this correlation, such that even when in your training data set. I think it was something like, you know, 60% correlation. So sort of 60% of cooking was forget which way it goes. It was either 60% of cooking was done by women. Or it was that.
- Yeah, I think it was that it was 60% of cooking was done by women. And then in the test data, even though sort of that percentage was still holding only 60% of cooking was done by women. The model actually predicted something like 80% of cooking images. So whenever it saw a cooking image, it was asked to predict the gender of the person, it would predict that as women sort of 80% of the time. And so
- you know, you can think about different methods for actually training visual classifiers that would ignore such correlations. And I'm not gonna
- dive into all of the technical details here, but there's a couple of papers that we did a few years ago, one that really looks at a wide range of methods. So what are some of the things you could do? You could like
- oversample some of these minority?
- so you would overstample sort of pictures of men that are cooking, or pictures of women that are doing other activities. You could consider training classifiers separately for sort of cooking, for like women cooking and men cooking as sort of 2 separate classifiers, and then combine the 2 at the end. You could also do things like generate data. So there's more and more work on data generation. So you could imagine sort of supplementing
- your potentially biased data set with some generated data to try to mitigate that.
- All of this is very, very interesting kind of technical work. But also this is a great example of something that you really can't do
- with just an engineering perspective like, you really need to be thinking a lot more deeply about as you're mitigating some of these biases? Are you introducing any additional ones in particular? If you're working with generated data? Is your generated data really helping
- sort of deviate your classifier introducing additional buy-ins.
- But
- yeah, and so and so, all of this sort of algorithms mitigating this correlation sort of becomes increasingly challenging, particularly in real data. Right? So these are some of the photos from a common computer vision data sets. And again, the photos are just so diverse that it's it's actually very hard to figure out. You know, when you're talking about like a face data set, you could say, Okay, what is the skin color of the person in the image, and that's sort of the dominant
- feature and sort of the dominant part of the appearance of the of the images. And so you can sort of run some analysis on that. But when you're working with very diverse images, it becomes increasingly harder to sort of understand what might be hiding in the data.
- All of this is sort of still possible and mitigating. This correlation is still possible. Assuming that you actually have the data, but algorithmically mitigating bias in the data actually becomes impossible when data is simply missing. And so one example of this is when people start talking about geographic diversity of computer vision data sets. So what I'm showing you here on the left is a map of the world
- where darker sort of darker shading corresponds to more images from that country appearing in some standard computer vision data sets. So you can see that, for example, like
- in these data sets, the majority of photos are collected from the Us. So so these are photos scraped from the web. But like we have some GPS tags on them, so we know where they were taken, or at least where they were uploaded from. And the vast majority of photos in this data set come from the Us.
- Some from Canada, like some from Australia, some from Europe, but then, like
- entire continent of Africa, is essentially not included in these data, sets right entire, like parts of Middle East. Parts of Asia, right? Are like, entirely excluded from this from these data sets. And
- One classic example for why this matters is that
- folks in 2017 showed that with some of the models back, then those models sort of again, commercial, you know, computer vision models were able to recognize us. Brands of liquid soap as soap. But we're not able to recognize pictures of bar soap as soap.
- So for this, you know, common household object of soap. We have it in. Like all of our houses, people around the world, the vast majority of households are going to have a bar soap rather than liquid soap. But in the Us. Particular sort of us brands of liquid soap. This is what primarily appeared in these data sets. And so this is primarily what the models learned from. And so this is primarily what they learned to recognize as soap.
- that would sort of diversify the geographic distribution of of
- objects that we collected this like geographically diverse object recognition data sets. It's 60,000 images. It's not very big by modern standards. So current, you know, computer vision models are trained on many, many millions of, you know, hundreds of millions, if not billions of images. But it's 60,000 images, 40 objects. And it was collected by partnering with people sort of around the world. So we hired photographers to go out and like.
- take, like basically hired people, to take pictures of household objects in their homes and collected this like very interesting data set, which we call geode. If you're, you know, interested in playing around with this, it's publicly available. So you can feel free.
- And one of the things that sort of was particularly interesting is so on the very bottom here. I'm showing pictures from Imagenet which is sort of a common computer vision data set. And then in the other sort of 6 rows, or I guess the sort of 6 groups above it are pictures of pictures from sort of 6 different regions of the world that we collected data from. And this is for the category of stove.
- And one of the things that you could see right is that
- when you focus on some of these images that I'm sort of highlighting yellow like they're very different than some of the, you know, kind of standard Us. Stoves that you might see in these data sets. So they are, for example, like single burner stoves, right? A lot of the stoves are a lot more used than when you collect Internet images. So when you scrape the web for photos of stoves, right? A lot of the time like, why was this picture uploaded to the web? Well.
- it was uploaded either to like sell the stove or you're trying to like
- showcase this object in some way, like you're trying to, you know. And so a lot of the time they are clean, they're particularly sort of staged, whereas if you just ask people to like, go out and take pictures of like, what does those look like in your house right now? You're going to get a very different very different distribution.
- which makes this sort of a lot
- a lot more interesting to kind of play around with. And I think you can start to see
- what are some of the challenges that current models that are trained particularly and almost exclusively on sort of web scrape data. What are some of the challenges that these models would actually encounter in the real world. If you look at sort of what actually, what actually exists in the real world versus what is captured in this state.
- So here. This is you know the model classifying a photo as a or sorry the other way around. These are pictures of houses from regions around the world. And then underneath each picture sort of what was it classified as by the model? And so you can look in like West Asia right? Both of these. These are houses? We asked. You know, we asked people to just take pictures of their houses, and both of these are classified as religious buildings by the model. Look at Africa, right?
- Classified as cooking pot and car. So like the model really has no idea what to do with these East Asia, you know. Front door. Okay? Fine. Fair enough, like front door. Okay, but like wheelbarrow, like what? And so forth.
- So again, even with like a very small data. You can actually reveal a lot of interesting patterns about some of these very large scale and seemingly very complex computer vision models.
- And so you know.
- coming back to this right? So so what can be done? Right? So so step one is still still know your data. Right?
- Collecting more represented data sets is really important, but also can be very difficult and very expensive to scale. So that's 1 of the sort of challenges within the AI space that's frequently under
- sort of appreciated. But it's very, very important. It's like knowing how to work with data and like knowing how to being able to and be willing to do some of this data collection work. And then the algorithmic interventions. Right? You can do stuff with that. But it also has limits. Again, for example, when you just simply don't have data from a certain part of the world, it's actually very hard to mitigate that using algorithmic interventions.
- So one more thing I sort of want to say is this idea of like being able to trace error pathways within the model. So if we think about sort of the cycle where human history, bias and prejudice like makes its way into large scale data in some form. So the data may be just simply reflecting patterns that exist in the world may simply be reflecting the fact that women tend to be doing more of the cooking like
- that is true. It may be reflecting that it may be amplifying that that sort of makes its way into the models that we build which are learning from the data. Again, the models might just be capturing the patterns and data. It may be amplifying patterns in the data that makes its way into AI decision making or sort of AI models that we deploy, and that becomes part of our human history.
- And so one question we can ask, right is sort of, we're in the cycle do these
- kind of biased behaviors or unfortunate behaviors common? So again, we talked about the gender and racial bias and face recognition. So this is the gender shades work where they analyzed sort of 3 commercial computer vision systems and looked at error rates between darker females and lighter males in particular. And so this can be sort of loosely traced back to imbalanced skin color distribution and face data sets.
- As I mentioned, we look at geographic bias in object recognition. This can sort of loosely be traced back to imbalanced geographic distribution and object data sets.
- sort of bias in the day so so like
- one example, right is, if you are
- So supposing you're trying to train a model to distinguish between like different animals out in the safari. Right? You're trying to distinguish like you're trying to classify different images. Maybe you have a camera that's placed. You know.
- I forgot the name for it, but but like a camera that's placed out in the wild to monitor sort of animal populations you're trying to distinguish between different types of animals. And suppose you're only taking sort of photos or videos in daylight. And so one of the like biases in your data is going to be that. Well, all of the images are captured in daylight, or that, you know the sky is always blue in this data, right? Like that is a sort of a bias in the day. That is part of the sort of patterns in the data. But like.
- is your AI model that's trained to classify different animals. Is that going to capture that bias like? Actually, probably not because it's sort of
- to associate sort of nighttime with some animals, and daytime with other images, with other animals, because that's going to be a very, very salient pattern in the day. Now, is it a problem? Maybe maybe not, because, like maybe some animals really do only come out at night, and you never need to recognize this animal during the day, and if you see something that looks like that animal during the day, you can like very safely assume it's probably not that animal, because those animals only come out at night. So this may actually not be a problem, or
- it might be a problem. If then you try to deploy this system in to recognize animals in the daytime, or you're trying to deploy it to like.
- I don't know. Assist a veterinarian and classify. You know what animal is this? And then you're going to bring in animals during that are all in like well lit daytime conditions right?
- The model may not work because it's sort of learned to make this daytime and nighttime association. So anyways, okay.
- so this is sort of the kinds of questions you can ask in the kind of like very technical
- things you can. You can dive into if you're if you're interested in this.
- But I want to sort of zoom out and come back in the last sort of few minutes of this talk to this claim about that. I said at the beginning that sort of AI is really about like data and models and humans. And so I want to come back to this question of sort of who are the humans involved in the data sets. And you know, I think, besides, the fact of like
- who is being represented in the data again, as we've talked about, or which countries or which cultures are represented. Right? You can also ask a number of questions about sort of the subjects of like, you know who is represented and how they represented. You can also ask the question like, did they consent to be included in this data?
- And finally, I do want to conclude by asking, sort of the Who are the AI researchers. And this is a big part of what Stanford AI. For all is all about right is sort of thinking about who is
- represented among AI developers. Among AI leaders there is a diversity crisis within AI. And this is, you know, some of the things that are easy to look at, our gender demographics or racial demographics. Those are easy to look at, and we can sort of
- talk about those and collect those statistics more easily. We're about 16% women within among AI researchers, right? And compared to world population of 50, 50, or even 51, 49 you could look at, you know, particular racial groups. You can compare that to populations of
- the Us. For example, but also even broader than that. Actually, there are things that are maybe hard to measure, but actually at least equally important about, you know, who are the people who are actually able to break into the field and become leaders in the field. What books did they read as children? What values do they hold? What communities do they come from? Like what life experiences have they had? And how does that factor? Into
- what models that they're building and what systems they're deploying and what questions they're asking.
- And you know one example that I do like to talk about here. And if you watch code of bias, you'll hear about that. But this work on gender shades of sort of questioning, you know. Do face recognition models work on all people of different skin color. I mean, this work was done by joy and meat, who are both black women, and this work was motivated by their lived experience of face recognition systems did not work for them, and you know joy talks about this example where she takes a white
- face mask just like a white, like sort of ghost mask, and puts it over her face, and the computer vision model recognizes that as a face, whereas when she takes it away and it's just her dark face.
- the the model does not recognize that as a face. And so this is something that sort of she lived with and has, or lived with, and is, you know, knows that this is how these models work. And so then they asked this question, sort of brought attention to this issue, and
- you know I have a sort of a minor scale experience with this. But like, when I started my Phd program, we were building a robot in my lab that was supposed to respond to commands, and the robot did not understand when I spoke to it, and understood every single other person in the lab. I. So I wasn't born in the Us, like English, is not technically my 1st language. But I've been in the Us since I was 10. And so it's
- basically become my 1st language. Lots of people in my lab were, you know, had like just come to the Us. Were just learning English had very thick accents, but the robot understood all of them, and would not understand me, and of course you might have guessed that, like I was the only woman in the lab, it would not understand, like female voices. And so you know, another great example of like, if we
- only have people who are building the systems who come from a certain subset of a population. And whether this be, you know, demographic or this be training, or this be value systems, etc, like, we just don't think to ask the right questions right? We think, to ask questions, you know some subset of questions, but not all of them. And so I do want to give a shout out, of course, to AI, for all. I suspect you
- heard about it, after all. But we are a nonprofit dedicated to increasing diversity, inclusion, and AI, and we celebrated our 8th birthday on March 8th of this year. So March 8, th for those who don't know is International Women's Day. So we were founded on International Women's Day. But our goal is to educate and support a diverse next generation of AI leaders, and we like to say that AI will change the world. And we like to ask the question of Who will change? AI,
- and we've developed a number of educational programs at the high school and college level. So including Stanford AI for all, including Princeton, AI. For all which we're starting in just a couple of weeks here.
- and many others for students from historically underrepresented and underserved communities were not currently represented among AI leaders, and one of my favorite quotes, this is from May, April 2016 student is that until this program I never thought that people who look like me could succeed in computer science. And AI, I think this is one of the most powerful quotes that I've heard, and this is something I'm very proud of, that we are, you know, dispelling some of the notions of what it means to be
- successful and a leader in AI. And I'm hoping that part of what you're getting out of this program is a sense that, like you, whoever you are wherever you're coming from, sort of whatever your background is, whatever your identity is that hopefully, you are starting to see yourself as somebody who can contribute and succeed in this space.
- and that is my
- greatest hope for you here at Stanford. AI for all. So just quick, takeaways things that we've talked about. Computer vision is no longer just about models or even the models plus data. It's really models plus data plus humans
- So thanks very much. I'm happy to take questions.
- Yeah, go ahead, Jasmine, jump, jump on in. Saw you first.st
- 00:48:07
	Hi, I have a question about like mitigating bias in this. Do you think that bias can ever be truly mitigated? Just by I mean manipulating the data set, since, like the data comes from like a society that's
- like, inherently and just deeply biased and unequal. And also when someone like manipulates the data set to mitigate bias, wouldn't. They're kind of
- biases. Be kind of thrown in there.
- 00:48:35
	A 100%. No to the 1st one. Yes, to the first, st to the second one. No, you can't have a truly unbiased data set just like you. I think it's
- very hard to have, or impossible to have, a truly, you know, unbiased society, or a truly unbiased person. But I think we can do better than we're doing, and I think we can be a lot more thoughtful about it. But no, we're not going to get this perfectly right and 100% agree with you that of course, when we're doing any kind of like bias mitigation work, we're introducing our own biases into it. Yes.
- the world or the data that you're seeing, or sort of societal phenomenon. A lot of the time will include like positionality statements at the end of their paper, saying, like, Here's who we are. And here's how our identities and perspectives might have contributed to this work. And and sort of do a number of other things to try to
- not even compensate for that. But just be upfront about it. And I think computer scientists could stand to learn a lot from that. There are disciplines that do much better job of this than we do. But yes, the core idea is, yes, 100%. We're bringing our own biased perspectives into that.
- 00:50:04
	Thank you.
- 00:50:05
	Yes, of course, Theresa, I think you're next.
- I can't hear you. You seem to be unmuted, but we can't hear you.
- 00:50:21
	3, 6.
- 00:50:22
	No worries.
- 00:50:22
	Do you mind writing your question? And then maybe I can read it out loud.
- 00:50:26
	Yep, yeah, Max, do you want to jump in.
- 00:50:32
	Sure. Hi, Professor Rosakowski, so it's nice to meet you. I want to ask you when real world data is limited. Do you think that synthetic images can help improve representation in data sets, because, like obviously, real data is much better. But if it's very scarce, then should we try to use synthetic data.
- 00:50:50
	Yes, absolutely. That's a great question, and absolutely. And we have some work on that, as one of the papers that I sort of very briefly showed, is where we're trying to use synthetic data. A lot of the same questions about like bias in the data very much applies to synthetic data. But on the plus side, there's things you can do with synthetic data, like sort of mixing
- things together and breaking correlations like quite effectively with synthetic data these days. So absolutely, that is a very, very good mitigate or potentially, like a very good mitigation strategy.
- 00:51:22
	Thank you.
- 00:51:23
	Nope.
- Earl Han, do you want to go next? Or I guess Miriam jump in whenever there is done with the question.
- 00:51:31
- 00:51:34
	Okay.
- 00:51:34
	Teresa's asking many models now pursue data efficiency due to scarce high quality data which makes me think about whether prioritizing data quantity to reduce bias conflict with data, efficient design and might standardized frameworks reconcile this tension. For instance, in medical imaging, should we accept higher algorithmic inaccuracy with minimal data to avoid reinforcing
- historical biases or prioritize large data sets, despite their potential to embed disparities.
- 00:52:07
	Oh, beautiful question.
- Oh, my God, yeah, yes and no to all of that like, yes, I mean, there's a lot of work now on, I mean.
- yes, there's sort of the the domains where there's just just very little data.
- And I think the questions then come is like, Do we need AI in those settings. Like, if if we're talking about sort of like. And this is part of you know, let's have AI replace doctors. Sure. I mean, maybe for a cold, where all you need to do, anyways, and go home and drink water like maybe that's fine that that like we give. You know, we build an AI system that tells you that. But like there's so many like long tail and rare events where you know, it's not actually clear that we need
- AI like what we really need is sort of somebody who's seen this at least once before. Right? It's it's not clear that we can really draw patterns from these like very long tail or rare events. But maybe we just sort of need
- need to try to be, you know, basically that that may not be appropriate for for a statistical model, even at all. And so I think that that's sort of one question is like, what happens is, it's very, very rare data examples. I think there's
- the other question of like moving towards data efficiency. And like.
- sort of, do we need Internet scale data? Or can we simplify this down? So one of the papers that we are just about to sort of put out on archive publicly is actually looking at sort of what happens. And do biases get actually exacerbated when you try to like, shrink down your data. And, of course, sort of? The answer is, you know, yes, in some
- circumstances, when you're you have a large scale data, but then you try to like, shrink it down to a smaller subset. Maybe your accuracy of your model can stay just as high. But actually, if you look at like robustness or performance of minority populations that can actually drop. So anyways, like
- all very, very good questions. And then the the question about, like, you know, prioritizing sort of
- high quality data versus like low quality data that may be more plentiful, that it versus like synthetic data that may even be more plentiful, but even lower quality. And then, like prioritize sort of model development versus data development. I mean, yes, to all of those. Let's prioritize all of this. This is why we need more people in the community working on all of these. Yeah, I don't think I have a good answer for like trade-offs there. But yeah, very, very good question. Sort of line of thought.
- Yeah, Rohan, do you want to jump in.
- 00:54:35
	Of course. So considering that many computer vision data sets are like fundamentally biased at the point of collection, right race gender geography, as you said, do you think it's more effective to fix these biases through like post hoc, algorithmic solutions like reweighting or like de-biasing models? Or should we focus on like, I guess, curating better data sets from the ground up.
- 00:54:57
	I think both like honestly, I think both. I think a lot of the time is there's some things that can be fixed algorithmically. And there's some things that just can't be. And so I think things that can be fixed through reweighting great. But I think those are a subset of the kinds of things that appear in there. So I think when we're thinking, I think reweighting is sort of a good like
- 1st step
- but in order to do that, you sort of need, basically need discrete labels and focus on like a small set of things. But then you're not going to fix things like grooms are represented as like white men in Texas. Next to white women and giant puffy like white dresses. Right? You're not going to fix that through waiting. If that's what if that's the only thing that's in your data.
- 00:55:45
	Thank you.
- 00:55:46
	Yeah.
- Come on.
- 00:55:50
	Hi, nice to meet you. I have a question about what the limitations of synthetic data are, and like, what exactly about it? Defines it as like a lower quality than real data.
- 00:56:00
	Yeah, also a wonderful question, like, what are the limitations of synthetic data? Right? So synthetic data? Generally, I mean, there's many different kinds. But a lot of the time people these days are thinking of this as like the output of a generative AI model. So sort of
- images that are, that is, that are produced by an AI model. That this is not the only possibility, like, I think, in the past, synthetic data was more frequently sort of rendered use like kind of using. Like, you know, video game type engines. And you could like render data. That, I think, is a more, much more controllable way of doing data generation. The limits. So when it's sort of rendered
- using the computer graphics type techniques the limitations that it. It frequently does not look realistic. So so it can
- sort of help in some ways. But it may not look realistic. The limitations of generated data by AI models is basically whatever by like A, whatever bias was in the AI model. That was also like sort of the generation model was trained on Internet scale data. So now, when you're using it to generate data that's going to have sort of the same biases.
- The other is that some of these models are not very good at like
- things like physical consistency, so they can generate again images that may not be realistic. And then one of the other things is that these models are actually kind of hard to control. And so it may be, you may not quite be sure that sort of what you're generating is
- what you meant to be generating. And then and then finally, of course, like it may again be different than the real world domain where you're actually operating. So the photos generated may look different than instead of your real world photograph. So so all of these reasons. But yeah, very good question.
- Alexander.
- 00:57:51
	Hi, it's nice to meet you on a more personal note. I would like to ask what what was like a core memory of you working at the Princeton Visual AI lab.
- 00:58:03
	Oh, at Princeton. Visual AI lab.
- let me see if I can show you so sort of up on the
- Thank you. That was a great great personal question. I like it, Sammy.
- 00:59:08
	Professor I am going to. I'm so sorry we have to wrap up.
- 00:59:11
	Done. Okay.
- 00:59:12
- 00:59:24
	Yeah, absolutely. And and the and the the folks who have their hands raised may maybe also, you know, you can type them in and and add them.
- 00:59:31
	Please do, please do send those to me, and I'll send them over to the professor and thank you all. Please head over to office hours, and give our professor sort of a virtual round of applause, and thank you so much for your time.
- 00:59:45
	Yeah, thank you all. Thank you for having me.
- 00:59:48
	If you open the chat you'll see a million. Thank you.
- 00:59:51
	Yup! Yup, I I just I disconnected. Oh, my goodness! Oh, that's a
- so beautiful! You guys are such a wonderful community. This is this is just this is just heartwarming. Thank you for having me thank you for letting me be a part of your day.
- enjoy office hours, enjoy the rest of the program.
- 01:00:08
	Thank you so much, Professor.
- 01:00:10
	Thanks folks, Hi.
- 01:00:13
	Goodbye!